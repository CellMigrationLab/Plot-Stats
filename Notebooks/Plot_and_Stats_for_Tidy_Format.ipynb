{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **Plot and Stats for Tidy Format**\n",
        "---\n",
        "\n",
        "<font size = 4>Colab Notebook for Plotting data\n",
        "\n",
        "\n",
        "<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR2U8v9YoJcW"
      },
      "source": [
        "# **Part 0. Before getting started**\n",
        "---\n",
        "\n",
        "<font size = 5>**Important notes**\n",
        "\n",
        "---\n",
        "## Data Requirements for Analysis\n",
        "\n",
        "<font size = 4>For a successful analysis using this notebook, ensure your data meets the following criteria:\n",
        "\n",
        "## Notebook Data Format and Requirements Documentation\n",
        "\n",
        "This document details the prerequisites for data to be analyzed effectively within this notebook. Ensuring adherence to these guidelines will facilitate accurate and efficient data analysis.\n",
        "\n",
        "### File Format\n",
        "- **CSV**: Data should be in CSV (Comma-Separated Values) format, easily generated from spreadsheet applications (e.g., Excel, Google Sheets) or statistical software (e.g., R, Python).\n",
        "- **Copy and Paste**: Data can be directly copied and pasted from a spreedsheet software.\n",
        "\n",
        "### Data Structure: Tidy Format\n",
        "Data must follow the tidy data principles for optimal processing:\n",
        "- **Each Variable Forms a Column**: Every column represents a single variable.\n",
        "- **Each Observation Forms a Row**: Every row represents a single observation.\n",
        "- **Each Type of Observational Unit Forms a Table**: Different observational units should be in separate tables or clearly distinguishable.\n",
        "\n",
        "### Essential Columns\n",
        "Your dataset must include specific columns for analysis:\n",
        "- **Biological Repeat Column**: Identifies biological replicates. Names can vary (e.g., \"Repeat\", \"Bio_Replicate\") but must consistently identify each biological repeat.\n",
        "- **Condition Column**: Categorizes observations by experimental conditions or treatments. Names can vary (e.g., \"Condition\", \"Treatment\") but must provide clear, consistent categorization.\n",
        "\n",
        "### Data Preparation Tips\n",
        "- **Consistency and Clarity**: Ensure consistent and descriptive naming within \"Biological Repeat\" and \"Condition\" columns.\n",
        "- **Data Cleaning**: Address missing or erroneous entries in these essential columns to prevent analysis issues.\n",
        "\n",
        "### Column Naming Flexibility\n",
        "- The exact names of the \"Biological Repeat\" and \"Condition\" columns are flexible to fit various dataset structures and terminologies. You'll specify these columns when using the notebook.\n",
        "\n",
        "Adhering to these guidelines ensures your data is primed for the notebook's analytical capabilities, allowing for insightful comparisons across biological repeats and conditions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1. Prepare the session and load your data**\n",
        "--------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Play to install\n",
        "!pip -q install pandas scikit-learn\n",
        "!pip -q install plotly\n",
        "!pip -q install prettytable\n",
        "!pip -q install pmoss\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S_BZuYOQGo1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAP0ahCzn1V6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to load the dependancies\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import itertools\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import requests\n",
        "\n",
        "!pip freeze > requirements.txt\n",
        "\n",
        "\n",
        "# Function to calculate Cohen's d\n",
        "def cohen_d(group1, group2):\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var()\n",
        "    var2 = group2.var()\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
        "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
        "\n",
        "    # Estimating the number of chunks based on the provided chunk size\n",
        "    num_chunks = int(len(df) / chunk_size) + 1\n",
        "\n",
        "    # Create a tqdm instance for progress tracking\n",
        "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
        "        # Open the file for writing\n",
        "        with open(path, \"w\") as f:\n",
        "            # Write the header once at the beginning\n",
        "            df.head(0).to_csv(f, index=False)\n",
        "\n",
        "            for chunk in np.array_split(df, num_chunks):\n",
        "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "def check_for_nans(df, df_name):\n",
        "    \"\"\"\n",
        "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
        "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
        "    \"\"\"\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            nan_count = df[col].isna().sum()\n",
        "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"No NaN values found in {df_name}.\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def save_parameters(params, file_path, param_type):\n",
        "    # Convert params dictionary to a DataFrame for human readability\n",
        "    new_params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])\n",
        "    new_params_df['Type'] = param_type\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        # Read existing file\n",
        "        existing_params_df = pd.read_csv(file_path)\n",
        "\n",
        "        # Merge the new parameters with the existing ones\n",
        "        # Update existing parameters or append new ones\n",
        "        updated_params_df = pd.merge(existing_params_df, new_params_df,\n",
        "                                     on=['Type', 'Parameter'],\n",
        "                                     how='outer',\n",
        "                                     suffixes=('', '_new'))\n",
        "\n",
        "        # If there's a new value, update it, otherwise keep the old value\n",
        "        updated_params_df['Value'] = updated_params_df['Value_new'].combine_first(updated_params_df['Value'])\n",
        "\n",
        "        # Drop the temporary new value column\n",
        "        updated_params_df.drop(columns='Value_new', inplace=True)\n",
        "    else:\n",
        "        # Use new parameters DataFrame directly if file doesn't exist\n",
        "        updated_params_df = new_params_df\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    updated_params_df.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDAwkSOo1gV"
      },
      "source": [
        "## **1.3. Load your dataset**\n",
        "---\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Load your dataset:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from io import StringIO\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Initialize dataset_df as an empty DataFrame globally\n",
        "dataset_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# Create widgets\n",
        "dataset_path_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to your dataset',\n",
        "    description='Dataset Path:',\n",
        "    layout={'width': '100%'}\n",
        ")\n",
        "\n",
        "results_folder_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to your results folder',\n",
        "    description='Results Folder:',\n",
        "    layout={'width': '100%'}\n",
        ")\n",
        "\n",
        "data_textarea = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Or copy and paste your tab sperated data here (direct copy and paste from a spreedsheet)',\n",
        "    description='Or Paste Data:',\n",
        "    layout={'width': '100%', 'height': '200px'}\n",
        ")\n",
        "\n",
        "load_button = widgets.Button(\n",
        "    description='Load Data',\n",
        "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to load the data',\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "# Load data function\n",
        "def load_data(b):\n",
        "    global dataset_df\n",
        "    global Results_Folder\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "        Results_Folder = results_folder_input.value.strip()\n",
        "        if not Results_Folder:\n",
        "            Results_Folder = './Results'  # Default path if not provided\n",
        "        if not os.path.exists(Results_Folder):\n",
        "            os.makedirs(Results_Folder)  # Create the folder if it doesn't exist\n",
        "        print(f\"Results folder is located at: {Results_Folder}\")\n",
        "\n",
        "        if dataset_path_input.value.strip():\n",
        "            dataset_path = dataset_path_input.value.strip()\n",
        "            try:\n",
        "                dataset_df = pd.read_csv(dataset_path)\n",
        "                print(f\"Loaded dataset from {dataset_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load dataset from {dataset_path}: {e}\")\n",
        "        elif data_textarea.value.strip():\n",
        "            input_data = StringIO(data_textarea.value)\n",
        "            try:\n",
        "                dataset_df = pd.read_csv(input_data, sep='\\t')\n",
        "                print(\"Loaded dataset from pasted tab-separated data\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load dataset from pasted data: {e}\")\n",
        "        else:\n",
        "            print(\"No dataset path provided or data pasted. Please provide a dataset.\")\n",
        "            return\n",
        "\n",
        "        # Perform a check for NaNs or any other required processing here\n",
        "        check_for_nans(dataset_df, \"your dataset\")\n",
        "\n",
        "        display(dataset_df.head())\n",
        "\n",
        "# Set the button click event\n",
        "load_button.on_click(load_data)\n",
        "\n",
        "# Display the widgets\n",
        "display(widgets.VBox([dataset_path_input, results_folder_input, data_textarea, load_button, output]))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yIX1uUc3NpCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4. Map your data**\n",
        "---\n",
        "\n",
        "## Required Columns\n",
        "\n",
        "<font size = 4>To plot your data, we need to ensure the presence of specific columns in the dataset. Here's a breakdown of the required columns:\n",
        "\n",
        "- **`Condition`**: Identifies the biological condition.\n",
        "\n",
        "- **`Repeat`**: Represents the biological repeat.\n",
        "\n",
        "- **`Variable`**: Represents the variable to plot.\n",
        "\n"
      ],
      "metadata": {
        "id": "dr9Wm3BIHnuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Map your dataset:\n",
        "\n",
        "\n",
        "import ipywidgets as widgets  # Ensure we have the required widgets module imported\n",
        "import pandas as pd\n",
        "\n",
        "def single_stage_column_mapping(df):\n",
        "    # Define the columns we need to map: Condition, Repeat\n",
        "    mappings = {\n",
        "        'Condition': 'Identifies the biological conditions.',\n",
        "        'Repeat': 'Represents the biological repeats.'\n",
        "    }\n",
        "\n",
        "    dropdowns = {}\n",
        "    for key, description in mappings.items():\n",
        "        description_label = widgets.Label(f\"{key} ({description}):\")\n",
        "        dropdowns[key] = widgets.Dropdown(options=df.columns, layout=widgets.Layout(width='250px'))\n",
        "\n",
        "        # Use HBox to display the description label next to the dropdown\n",
        "        hbox = widgets.HBox([description_label, dropdowns[key]])\n",
        "        display(hbox)\n",
        "\n",
        "    confirm_button = widgets.Button(description=\"Confirm Mappings\")\n",
        "\n",
        "    def confirm_mappings(button):\n",
        "        # Perform the mapping based on the user selection\n",
        "        column_mapping = {dropdown.value: key for key, dropdown in dropdowns.items()}\n",
        "        new_df = df.rename(columns=column_mapping)\n",
        "\n",
        "        print(\"Columns Mapped Successfully!\")\n",
        "\n",
        "        # Count and print unique conditions\n",
        "        unique_conditions = new_df['Condition'].unique()\n",
        "        print(f\"Number of unique conditions: {len(unique_conditions)}\")\n",
        "        print(\"Conditions:\", \", \".join(unique_conditions))\n",
        "\n",
        "        # Count and print biological repeats\n",
        "        unique_repeats = new_df['Repeat'].unique()\n",
        "        print(f\"Number of biological repeats: {len(unique_repeats)}\")\n",
        "        print(\"Repeats:\", \", \".join(map(str, unique_repeats)))\n",
        "\n",
        "\n",
        "        # Check that each biological condition has exactly the same repeat names\n",
        "        condition_repeats = new_df.groupby('Condition')['Repeat'].apply(set)\n",
        "        if len(set(map(frozenset, condition_repeats))) == 1:\n",
        "            print(\"All biological conditions have exactly the same repeat names.\")\n",
        "        else:\n",
        "            print(\"Warning: Not all biological conditions have the same repeat names.\")\n",
        "\n",
        "        # Update the global dataset_df with the new mappings\n",
        "        global dataset_df\n",
        "        dataset_df = new_df\n",
        "\n",
        "    confirm_button.on_click(confirm_mappings)\n",
        "    display(confirm_button)\n",
        "\n",
        "single_stage_column_mapping(dataset_df)\n"
      ],
      "metadata": {
        "id": "5VE9oqIEHrG-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11aD1AmQh7ST"
      },
      "source": [
        "-------------------------------------------\n",
        "\n",
        "# **Part 2. Plot your entire dataset**\n",
        "-------------------------------------------\n",
        "\n",
        "<font size = 4> In this section you can plot your data. Data and graphs are automatically saved in your result folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pjRDlYOgr3r"
      },
      "source": [
        "## **2.1. Plot your entire dataset**\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Statistical analyses**\n",
        "### Cohen's d (Effect Size):\n",
        "<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n",
        "\n",
        "### Randomization Test:\n",
        "<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n",
        "\n",
        "### Bonferroni Correction:\n",
        "<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects."
      ],
      "metadata": {
        "id": "hIa_MlR2Ktu-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HTMbI68Heyis"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot (entire dataset)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.ticker import FixedLocator\n",
        "\n",
        "\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Plots\"\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = dataset_df\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "    # Select only numerical columns\n",
        "    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n",
        "\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def create_condition_selector(df, column_name):\n",
        "    conditions = df[column_name].unique()\n",
        "    condition_selector = SelectMultiple(\n",
        "        options=conditions,\n",
        "        description='Conditions:',\n",
        "        disabled=False,\n",
        "        layout=Layout(width='100%')  # Adjusting the layout width\n",
        "    )\n",
        "    return condition_selector\n",
        "\n",
        "def display_condition_selection(df, column_name):\n",
        "    condition_selector = create_condition_selector(df, column_name)\n",
        "\n",
        "    condition_accordion = Accordion(children=[VBox([condition_selector])])\n",
        "    condition_accordion.set_title(0, 'Select Conditions')\n",
        "    display(condition_accordion)\n",
        "    return condition_selector\n",
        "\n",
        "\n",
        "def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n",
        "\n",
        "    plt.clf()  # Clear the current figure before creating a new plot\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "  # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "  # Get selected conditions\n",
        "    selected_conditions = condition_selector.value\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "\n",
        "    if n_selected_conditions == 0:\n",
        "        print(\"No conditions selected for plotting\")\n",
        "        return\n",
        "\n",
        "# Use only selected and ordered conditions\n",
        "    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n",
        "\n",
        "# Initialize matrices to store effect sizes and p-values for each variable\n",
        "    effect_size_matrices = {}\n",
        "    p_value_matrices = {}\n",
        "    bonferroni_matrices = {}\n",
        "\n",
        "    unique_conditions = filtered_df[Conditions].unique().tolist()\n",
        "    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n",
        "    alpha = 0.05\n",
        "    corrected_alpha = alpha / num_comparisons\n",
        "    n_iterations = 1000\n",
        "\n",
        "# Loop through each variable to plot\n",
        "    for var in variables_to_plot:\n",
        "\n",
        "      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n",
        "      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "\n",
        "      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n",
        "        group1 = df[df[Conditions] == cond1][var]\n",
        "        group2 = df[df[Conditions] == cond2][var]\n",
        "\n",
        "        original_d = abs(cohen_d(group1, group2))\n",
        "        effect_size_matrix.loc[cond1, cond2] = original_d\n",
        "        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n",
        "\n",
        "        count_extreme = 0\n",
        "        for i in range(n_iterations):\n",
        "            combined = pd.concat([group1, group2])\n",
        "            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n",
        "            new_group1 = shuffled[:len(group1)]\n",
        "            new_group2 = shuffled[len(group1):]\n",
        "\n",
        "            new_d = abs(cohen_d(new_group1, new_group2))\n",
        "            if np.abs(new_d) >= np.abs(original_d):\n",
        "                count_extreme += 1\n",
        "\n",
        "        p_value = count_extreme / n_iterations\n",
        "        p_value_matrix.loc[cond1, cond2] = p_value\n",
        "        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n",
        "\n",
        "        # Apply Bonferroni correction\n",
        "        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n",
        "        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n",
        "        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n",
        "\n",
        "      effect_size_matrices[var] = effect_size_matrix\n",
        "      p_value_matrices[var] = p_value_matrix\n",
        "      bonferroni_matrices[var] = bonferroni_matrix\n",
        "\n",
        "    # Concatenate the three matrices side-by-side\n",
        "      combined_df = pd.concat(\n",
        "        [\n",
        "            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n",
        "            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n",
        "            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n",
        "        ], axis=1\n",
        "    )\n",
        "\n",
        "    # Save the combined DataFrame to a CSV file\n",
        "      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n",
        "\n",
        "    # Create a new figure\n",
        "      fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Create a gridspec for 2 rows and 4 columns\n",
        "      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n",
        "\n",
        "    # Create the ax for boxplot using the gridspec\n",
        "      ax_box = fig.add_subplot(gs[0, :])\n",
        "\n",
        "    # Extract the data for this variable\n",
        "      data_for_var = df[[Conditions, var, 'Repeat' ]]\n",
        "\n",
        "    # Save the data_for_var to a CSV for replotting\n",
        "      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n",
        "\n",
        "    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n",
        "      Q1 = df[var].quantile(0.25)\n",
        "      Q3 = df[var].quantile(0.75)\n",
        "      IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "      multiplier = 10\n",
        "      lower_bound = Q1 - multiplier * IQR\n",
        "      upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "    # Plotting\n",
        "      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n",
        "      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n",
        "      ax_box.set_title(f\"{var}\")\n",
        "      ax_box.set_xlabel('Condition')\n",
        "      ax_box.set_ylabel(var)\n",
        "      tick_labels = ax_box.get_xticklabels()\n",
        "      tick_locations = ax_box.get_xticks()\n",
        "      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n",
        "      ax_box.set_xticklabels(tick_labels, rotation=90)\n",
        "      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "    # Statistical Analyses and Heatmaps\n",
        "\n",
        "    # Effect Size heatmap ax\n",
        "      ax_d = fig.add_subplot(gs[1, 0])\n",
        "      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n",
        "      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n",
        "\n",
        "    # p-value heatmap ax\n",
        "      ax_p = fig.add_subplot(gs[1, 1])\n",
        "      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n",
        "      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n",
        "\n",
        "    # Bonferroni corrected p-value heatmap ax\n",
        "      ax_bonf = fig.add_subplot(gs[1, 2])\n",
        "      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n",
        "      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      pdf_pages.savefig(fig)\n",
        "\n",
        "    # Close the PDF\n",
        "      pdf_pages.close()\n",
        "\n",
        "condition_selector = display_condition_selection(df_to_plot, Conditions)\n",
        "selectable_columns = get_selectable_columns(df_to_plot)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n",
        "display(button)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. Export data summaries**\n",
        "--------"
      ],
      "metadata": {
        "id": "vLn_OF4689rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "import os\n",
        "\n",
        "# @title ##Export the data summaries\n",
        "\n",
        "\n",
        "# Assuming Results_Folder, dataset_df, and Conditions are defined\n",
        "save_path = f\"{Results_Folder}/variables_summary\"\n",
        "df_to_plot = dataset_df  # Assuming dataset_df is the DataFrame to work with\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "\n",
        "def generate_display_and_save_statistics(df, columns, Conditions, save_path):\n",
        "    \"\"\"\n",
        "    Generates, displays using prettytable, and saves as CSV the statistical summaries\n",
        "    for selected columns of the DataFrame, grouped by the specified condition column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to analyze.\n",
        "    - columns: List of column names to generate statistics for.\n",
        "    - Conditions: Column name to group by.\n",
        "    - save_path: Directory path where CSV files will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the save directory exists\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for var in columns:\n",
        "        if var in df.columns:\n",
        "            # Compute descriptive statistics and additional metrics\n",
        "            grouped_stats = df.groupby(Conditions)[var].describe()\n",
        "            variance = df.groupby(Conditions)[var].var().rename('variance')\n",
        "            skewness = df.groupby(Conditions)[var].skew().rename('skewness')\n",
        "            kurtosis = df.groupby(Conditions)[var].apply(pd.DataFrame.kurt).rename('kurtosis')\n",
        "\n",
        "            # Concatenate all statistics into a single DataFrame\n",
        "            all_stats = pd.concat([grouped_stats, variance, skewness, kurtosis], axis=1)\n",
        "\n",
        "            # Save the summary to a CSV file\n",
        "            csv_filename = f\"{var}_summary.csv\"\n",
        "            all_stats.to_csv(os.path.join(save_path, csv_filename))\n",
        "            print(f\"Saved statistical summary for {var} to {csv_filename}\")\n",
        "\n",
        "            # Initialize PrettyTable, using the DataFrame's columns as table fields\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"Condition\"] + list(all_stats.columns)\n",
        "\n",
        "            # Populate the table with data\n",
        "            for condition, row in all_stats.iterrows():\n",
        "                table.add_row([condition] + row.tolist())\n",
        "\n",
        "            # Set table alignment, style, etc.\n",
        "            table.align = 'r'\n",
        "            print(f\"Statistical Summary for {var}:\\n{table}\")\n",
        "\n",
        "generate_display_and_save_statistics(df_to_plot, selectable_columns, Conditions, save_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7ycQAd7HKP3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3. pMoSS (p-value Model using the Sample Size)** (To implement)\n",
        "--------"
      ],
      "metadata": {
        "id": "Fmnkq5cG9IDv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHi3-zp3JTd"
      },
      "source": [
        "--------\n",
        "# **Part 3. Plot a balanced dataset (batch correction)**\n",
        "--------\n",
        "\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. Assess if your dataset is balanced**\n",
        "---\n",
        "\n",
        "In data analyses, the balance of the dataset is important, particularly in ensuring that each biological repeat carries equal weight. Here's why this balance is essential:\n",
        "\n",
        "### Accurate Representation of Biological Variability\n",
        "\n",
        "- **Capturing True Biological Variation**: Biological repeats are crucial for capturing the natural variability inherent in biological systems. Equal weighting ensures that this variability is accurately represented.\n",
        "- **Reducing Sampling Bias**: By balancing the dataset, we avoid overemphasizing the characteristics of any single repeat, which might not be representative of the broader biological context.\n",
        "\n",
        "If your data is too imbalanced, it may be useful to ensure that this does not shift your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "dqu5RWoh8TNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ##Check the number of values per condition per repeats\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "\n",
        "def count_values_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat'):\n",
        "    \"\"\"\n",
        "    Counts the number of rows for each combination of condition and repeat, plots a histogram,\n",
        "    and adds the counts on top of each bar in the graph.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the results folder exists\n",
        "    Balanced_dataset_folder = os.path.join(Results_Folder, \"Balanced_dataset\")\n",
        "    if not os.path.exists(Balanced_dataset_folder):\n",
        "        os.makedirs(Balanced_dataset_folder)\n",
        "\n",
        "    # Counting rows per condition and repeat\n",
        "    value_counts = df.groupby([condition_col, repeat_col]).size().reset_index(name='Number_of_Values')\n",
        "\n",
        "    # Preparing data for plotting\n",
        "    pivot_df = value_counts.pivot(index=condition_col, columns=repeat_col, values='Number_of_Values').fillna(0)\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel('Number of Values')\n",
        "    ax.set_title('Stacked Histogram of Value Counts per Condition and Repeat')\n",
        "    ax.legend(title=repeat_col)\n",
        "\n",
        "    # Adding counts on top of each bar\n",
        "    for bar in bars.patches:\n",
        "        height = bar.get_height()\n",
        "        if height > 0:  # Only annotate non-zero heights to avoid clutter\n",
        "            ax.annotate(f'{int(height)}',\n",
        "                        (bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n",
        "                        ha='center', va='center', color='white', size=9)\n",
        "\n",
        "    # Save the plot as a PDF in the QC subfolder\n",
        "    pdf_file = os.path.join(Balanced_dataset_folder, 'Value_Counts_Histogram.pdf')\n",
        "    plt.savefig(pdf_file, bbox_inches='tight')\n",
        "    print(f\"Saved histogram to {pdf_file}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming dataset_df is your DataFrame and Results_Folder is defined\n",
        "# result_df = count_values_by_condition_and_repeat(dataset_df, Results_Folder)\n",
        "\n",
        "result_df = count_values_by_condition_and_repeat(dataset_df, Results_Folder)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dlqqoWZlzoeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FJT4P_Xgr35"
      },
      "source": [
        "## **3.2. Downsample your dataset to ensure that it is balanced**\n",
        "--------\n",
        "\n",
        "### Downsampling and Balancing Dataset\n",
        "\n",
        "This section of the notebook is dedicated to addressing imbalances in the dataset, which is crucial for ensuring the accuracy and reliability of the analysis. The cell bellow will downsample the dataset to balance the number of tracks across different conditions and repeats. It allows for reproducibility by including a `random_seed` parameter, which is set to 42 by default but can be adjusted as needed.\n",
        "\n",
        "All results from this section will be saved in the Balanced Dataset Directory created in your `Results_Folder`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# @title ##Run this cell to downsample and balance your dataset\n",
        "\n",
        "random_seed = 42  # @param {type: \"number\"}\n",
        "\n",
        "if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n",
        "\n",
        "def balance_dataset(df, condition_col='Condition', repeat_col='Repeat', random_seed=None):\n",
        "    \"\"\"\n",
        "    Balances the dataset by downsampling rows for each condition and repeat combination.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame containing the data.\n",
        "    condition_col (str): The name of the column representing the condition.\n",
        "    repeat_col (str): The name of the column representing the repeat.\n",
        "    random_seed (int, optional): The seed for the random number generator. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A new DataFrame with balanced row counts.\n",
        "    \"\"\"\n",
        "    # Group by condition and repeat, and find the minimum row count for any group\n",
        "    min_row_count = df.groupby([condition_col, repeat_col]).size().min()\n",
        "\n",
        "    # Function to sample min_row_count rows from each group\n",
        "    def sample_rows(group):\n",
        "        return group.sample(n=min_row_count, random_state=random_seed)\n",
        "\n",
        "    # Apply sampling to each group and concatenate the results\n",
        "    balanced_df = df.groupby([condition_col, repeat_col], group_keys=False).apply(sample_rows)\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# Apply the function to your dataset\n",
        "balanced_dataset_df = balance_dataset(dataset_df, random_seed=random_seed)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qnJ7W-mCAih_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phuPCeabgr35"
      },
      "source": [
        "## **3.3. Check if the downsampling has affected data distribution**\n",
        "--------\n",
        "\n",
        "This section of the notebook generates a heatmap visualizing the Kolmogorov-Smirnov (KS) p-values for each numerical column in the dataset, comparing the distributions before and after downsampling. This heatmap serves as a tool for assessing the impact of downsampling on data quality, guiding decisions on whether the downsampled dataset is suitable for further analysis.\n",
        "\n",
        "#### Purpose of the Heatmap\n",
        "- **KS Test:** The KS test is used to determine if two samples are drawn from the same distribution. In this context, it compares the distribution of each numerical column in the original dataset (`merged_tracks_df`) with its counterpart in the downsampled dataset (`balanced_merged_tracks_df`).\n",
        "- **P-Value Interpretation:** The p-value indicates the probability that the two samples come from the same distribution. A higher p-value suggests a greater likelihood that the distributions are similar.\n",
        "\n",
        "#### Interpreting the Heatmap\n",
        "- **Color Coding:** The heatmap uses a color gradient (from viridis) to represent the range of p-values. Darker colors indicate higher p-values.\n",
        "- **P-Value Thresholds:**\n",
        "  - **High P-Values (Lighter Areas):** Indicate that the downsampling process likely did not significantly alter the distribution of that numerical column for the specific condition-repeat group.\n",
        "  - **Low P-Values (Darker Areas):** Suggest that the downsampling process may have affected the distribution significantly.\n",
        "- **Varying P-Values:** Variations in color across different columns and rows help identify which specific numerical columns and condition-repeat groups are most affected by the downsampling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FLVzJH3ogr36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# @title ##Check if your downsampling has affected your data distribution\n",
        "\n",
        "def calculate_ks_p_value(df1, df2, column):\n",
        "    \"\"\"\n",
        "    Calculate the KS p-value for a given column between two dataframes.\n",
        "\n",
        "    Parameters:\n",
        "    df1 (pandas.DataFrame): Original DataFrame.\n",
        "    df2 (pandas.DataFrame): DataFrame after downsampling.\n",
        "    column (str): Column name to compare.\n",
        "\n",
        "    Returns:\n",
        "    float: KS p-value.\n",
        "    \"\"\"\n",
        "    return ks_2samp(df1[column].dropna(), df2[column].dropna())[1]\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_columns = dataset_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Initialize a DataFrame to store KS p-values\n",
        "ks_p_values = pd.DataFrame(columns=numerical_columns)\n",
        "\n",
        "# Iterate over each group and numerical column\n",
        "for group, group_df in dataset_df.groupby(['Condition', 'Repeat']):\n",
        "    group_p_values = []\n",
        "    balanced_group_df = balanced_dataset_df[(balanced_dataset_df['Condition'] == group[0]) & (balanced_dataset_df['Repeat'] == group[1])]\n",
        "    for column in numerical_columns:\n",
        "        p_value = calculate_ks_p_value(group_df, balanced_group_df, column)\n",
        "        group_p_values.append(p_value)\n",
        "    ks_p_values.loc[f'Condition: {group[0]}, Repeat: {group[1]}'] = group_p_values\n",
        "\n",
        "# Maximum number of columns per heatmap\n",
        "max_columns_per_heatmap = 20\n",
        "\n",
        "# Total number of columns\n",
        "total_columns = len(ks_p_values.columns)\n",
        "\n",
        "# Calculate the number of heatmaps needed\n",
        "num_heatmaps = -(-total_columns // max_columns_per_heatmap)  # Ceiling division\n",
        "\n",
        "# File path for the PDF\n",
        "pdf_filepath = Results_Folder+'/Balanced_dataset/p-Value Heatmap.pdf'\n",
        "\n",
        "# Create a PDF file\n",
        "with PdfPages(pdf_filepath) as pdf:\n",
        "    # Loop through each subset of columns and create a heatmap\n",
        "    for i in range(num_heatmaps):\n",
        "        start_col = i * max_columns_per_heatmap\n",
        "        end_col = min(start_col + max_columns_per_heatmap, total_columns)\n",
        "\n",
        "        # Subset of columns for this heatmap\n",
        "        subset_columns = ks_p_values.columns[start_col:end_col]\n",
        "\n",
        "        # Create the heatmap for the subset of columns\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(ks_p_values[subset_columns], cmap='viridis', vmax=0.5, vmin=0)\n",
        "        plt.title(f'Kolmogorov-Smirnov P-Value Heatmap (Columns {start_col+1} to {end_col})')\n",
        "        plt.xlabel('Numerical Columns')\n",
        "        plt.ylabel('Condition-Repeat Groups')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the current figure to the PDF\n",
        "        pdf.savefig()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "print(f\"Saved all heatmaps to {pdf_filepath}\")\n",
        "\n",
        "# Save the p-values to a CSV file\n",
        "ks_p_values.to_csv(Results_Folder + '/Balanced_dataset/ks_p_values.csv')\n",
        "print(\"Saved KS p-values to ks_p_values.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2eUxF87gr36"
      },
      "source": [
        "## **3.4. Plot your balanced dataset**\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ssuVkgP4a3Hf"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot track parameters (balanced dataset)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib.ticker import FixedLocator\n",
        "\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Balanced_dataset/track_parameters_plots\"\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = balanced_dataset_df\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "    # Select only numerical columns\n",
        "    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n",
        "\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def create_condition_selector(df, column_name):\n",
        "    conditions = df[column_name].unique()\n",
        "    condition_selector = SelectMultiple(\n",
        "        options=conditions,\n",
        "        description='Conditions:',\n",
        "        disabled=False,\n",
        "        layout=Layout(width='100%')  # Adjusting the layout width\n",
        "    )\n",
        "    return condition_selector\n",
        "\n",
        "def display_condition_selection(df, column_name):\n",
        "    condition_selector = create_condition_selector(df, column_name)\n",
        "\n",
        "    condition_accordion = Accordion(children=[VBox([condition_selector])])\n",
        "    condition_accordion.set_title(0, 'Select Conditions')\n",
        "    display(condition_accordion)\n",
        "    return condition_selector\n",
        "\n",
        "\n",
        "def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n",
        "\n",
        "    plt.clf()  # Clear the current figure before creating a new plot\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "  # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "  # Get selected conditions\n",
        "    selected_conditions = condition_selector.value\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "\n",
        "    if n_selected_conditions == 0:\n",
        "        print(\"No conditions selected for plotting\")\n",
        "        return\n",
        "\n",
        "# Use only selected and ordered conditions\n",
        "    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n",
        "\n",
        "# Initialize matrices to store effect sizes and p-values for each variable\n",
        "    effect_size_matrices = {}\n",
        "    p_value_matrices = {}\n",
        "    bonferroni_matrices = {}\n",
        "\n",
        "    unique_conditions = filtered_df[Conditions].unique().tolist()\n",
        "    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n",
        "    alpha = 0.05\n",
        "    corrected_alpha = alpha / num_comparisons\n",
        "    n_iterations = 1000\n",
        "\n",
        "# Loop through each variable to plot\n",
        "    for var in variables_to_plot:\n",
        "\n",
        "      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n",
        "      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "\n",
        "      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n",
        "        group1 = df[df[Conditions] == cond1][var]\n",
        "        group2 = df[df[Conditions] == cond2][var]\n",
        "\n",
        "        original_d = abs(cohen_d(group1, group2))\n",
        "        effect_size_matrix.loc[cond1, cond2] = original_d\n",
        "        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n",
        "\n",
        "        count_extreme = 0\n",
        "        for i in range(n_iterations):\n",
        "            combined = pd.concat([group1, group2])\n",
        "            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n",
        "            new_group1 = shuffled[:len(group1)]\n",
        "            new_group2 = shuffled[len(group1):]\n",
        "\n",
        "            new_d = abs(cohen_d(new_group1, new_group2))\n",
        "            if np.abs(new_d) >= np.abs(original_d):\n",
        "                count_extreme += 1\n",
        "\n",
        "        p_value = count_extreme / n_iterations\n",
        "        p_value_matrix.loc[cond1, cond2] = p_value\n",
        "        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n",
        "\n",
        "        # Apply Bonferroni correction\n",
        "        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n",
        "        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n",
        "        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n",
        "\n",
        "      effect_size_matrices[var] = effect_size_matrix\n",
        "      p_value_matrices[var] = p_value_matrix\n",
        "      bonferroni_matrices[var] = bonferroni_matrix\n",
        "\n",
        "    # Concatenate the three matrices side-by-side\n",
        "      combined_df = pd.concat(\n",
        "        [\n",
        "            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n",
        "            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n",
        "            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n",
        "        ], axis=1\n",
        "    )\n",
        "\n",
        "    # Save the combined DataFrame to a CSV file\n",
        "      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n",
        "\n",
        "    # Create a new figure\n",
        "      fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Create a gridspec for 2 rows and 4 columns\n",
        "      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n",
        "\n",
        "    # Create the ax for boxplot using the gridspec\n",
        "      ax_box = fig.add_subplot(gs[0, :])\n",
        "\n",
        "    # Extract the data for this variable\n",
        "      data_for_var = df[[Conditions, var, 'Repeat' ]]\n",
        "\n",
        "    # Save the data_for_var to a CSV for replotting\n",
        "      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n",
        "\n",
        "    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n",
        "      Q1 = df[var].quantile(0.25)\n",
        "      Q3 = df[var].quantile(0.75)\n",
        "      IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "      multiplier = 10\n",
        "      lower_bound = Q1 - multiplier * IQR\n",
        "      upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "    # Plotting\n",
        "      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n",
        "      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n",
        "      ax_box.set_title(f\"{var}\")\n",
        "      ax_box.set_xlabel('Condition')\n",
        "      ax_box.set_ylabel(var)\n",
        "      tick_labels = ax_box.get_xticklabels()\n",
        "      tick_locations = ax_box.get_xticks()\n",
        "      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n",
        "      ax_box.set_xticklabels(tick_labels, rotation=90)\n",
        "      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "    # Statistical Analyses and Heatmaps\n",
        "\n",
        "    # Effect Size heatmap ax\n",
        "      ax_d = fig.add_subplot(gs[1, 0])\n",
        "      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n",
        "      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n",
        "\n",
        "    # p-value heatmap ax\n",
        "      ax_p = fig.add_subplot(gs[1, 1])\n",
        "      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n",
        "      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n",
        "\n",
        "    # Bonferroni corrected p-value heatmap ax\n",
        "      ax_bonf = fig.add_subplot(gs[1, 2])\n",
        "      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n",
        "      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      pdf_pages.savefig(fig)\n",
        "\n",
        "    # Close the PDF\n",
        "      pdf_pages.close()\n",
        "\n",
        "condition_selector = display_condition_selection(df_to_plot, Conditions)\n",
        "selectable_columns = get_selectable_columns(df_to_plot)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n",
        "display(button)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "import os\n",
        "\n",
        "# @title ##Export the data summaries\n",
        "\n",
        "\n",
        "# Assuming Results_Folder, dataset_df, and Conditions are defined\n",
        "save_path = f\"{Results_Folder}/Balanced_dataset/track_parameters_plots\"\n",
        "df_to_plot = balanced_dataset_df  # Assuming dataset_df is the DataFrame to work with\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "\n",
        "def generate_display_and_save_statistics(df, columns, Conditions, save_path):\n",
        "    \"\"\"\n",
        "    Generates, displays using prettytable, and saves as CSV the statistical summaries\n",
        "    for selected columns of the DataFrame, grouped by the specified condition column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to analyze.\n",
        "    - columns: List of column names to generate statistics for.\n",
        "    - Conditions: Column name to group by.\n",
        "    - save_path: Directory path where CSV files will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the save directory exists\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for var in columns:\n",
        "        if var in df.columns:\n",
        "            # Compute descriptive statistics and additional metrics\n",
        "            grouped_stats = df.groupby(Conditions)[var].describe()\n",
        "            variance = df.groupby(Conditions)[var].var().rename('variance')\n",
        "            skewness = df.groupby(Conditions)[var].skew().rename('skewness')\n",
        "            kurtosis = df.groupby(Conditions)[var].apply(pd.DataFrame.kurt).rename('kurtosis')\n",
        "\n",
        "            # Concatenate all statistics into a single DataFrame\n",
        "            all_stats = pd.concat([grouped_stats, variance, skewness, kurtosis], axis=1)\n",
        "\n",
        "            # Save the summary to a CSV file\n",
        "            csv_filename = f\"{var}_summary.csv\"\n",
        "            all_stats.to_csv(os.path.join(save_path, csv_filename))\n",
        "            print(f\"Saved statistical summary for {var} to {csv_filename}\")\n",
        "\n",
        "            # Initialize PrettyTable, using the DataFrame's columns as table fields\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"Condition\"] + list(all_stats.columns)\n",
        "\n",
        "            # Populate the table with data\n",
        "            for condition, row in all_stats.iterrows():\n",
        "                table.add_row([condition] + row.tolist())\n",
        "\n",
        "            # Set table alignment, style, etc.\n",
        "            table.align = 'r'\n",
        "            print(f\"Statistical Summary for {var}:\\n{table}\")\n",
        "\n",
        "generate_display_and_save_statistics(df_to_plot, selectable_columns, Conditions, save_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KRx9fZGbCeSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd817UHYhCGD"
      },
      "source": [
        "# **Part 4. Version log**\n",
        "---\n",
        "<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n",
        "  - This notebook may contain bugs.\n",
        "  - Features are currently limited and will be expanded in future releases.\n",
        "\n",
        "<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n",
        "\n",
        "\n",
        "<font size = 4>**Version 0.1**\n",
        "This is the first release of this notebook.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3Kzd_8GUnpbw",
        "FJHi3-zp3JTd"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}