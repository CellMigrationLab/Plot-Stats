{"cells":[{"cell_type":"markdown","metadata":{"id":"xF4zYMmXULP7"},"source":["# **Plot&Stats - dimensionality reduction**\n","---\n","\n","<font size = 4>Colab Notebook for generating PCA, UMAP or t-SNE dimensional reduction of multidimensional datasets.\n","\n","\n","<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aR2U8v9YoJcW"},"source":["# **Part 0. Before getting started**\n","---\n","\n"]},{"cell_type":"markdown","source":["##<font size = 5>**Important notes**\n","\n","---\n","## Data Requirements for Analysis\n","\n","<font size = 4>For a successful analysis using this notebook, ensure your data meets the following criteria:\n","\n","## Notebook Data Format and Requirements Documentation\n","\n","This document details the prerequisites for data to be analyzed effectively within this notebook. Ensuring adherence to these guidelines will facilitate accurate and efficient data analysis.\n","\n","### File Format\n","- **CSV**: Data should be in CSV (Comma-Separated Values) format, easily generated from spreadsheet applications (e.g., Excel, Google Sheets) or statistical software (e.g., R, Python).\n","- **Copy and Paste**: Data can be directly copied and pasted from a spreedsheet software.\n","\n","### Data Structure: Tidy Format\n","Data must follow the tidy data principles for optimal processing:\n","- **Each Variable Forms a Column**: Every column represents a single variable.\n","- **Each Observation Forms a Row**: Every row represents a single observation.\n","- **Each Type of Observational Unit Forms a Table**: Different observational units should be in separate tables or clearly distinguishable.\n","\n","### Essential Columns\n","Your dataset must include specific columns for analysis:\n","- **Biological Repeat Column**: Identifies biological replicates. Names can vary (e.g., \"Repeat\", \"Bio_Replicate\") but must consistently identify each biological repeat.\n","- **Condition Column**: Categorizes observations by experimental conditions or treatments. Names can vary (e.g., \"Condition\", \"Treatment\") but must provide clear, consistent categorization.\n","\n","### Data Preparation Tips\n","- **Consistency and Clarity**: Ensure consistent and descriptive naming within \"Biological Repeat\" and \"Condition\" columns.\n","- **Data Cleaning**: Address missing or erroneous entries in these essential columns to prevent analysis issues.\n","\n","### Column Naming Flexibility\n","- The exact names of the \"Biological Repeat\" and \"Condition\" columns are flexible to fit various dataset structures and terminologies. You'll specify these columns when using the notebook.\n","\n","Adhering to these guidelines ensures your data is primed for the notebook's analytical capabilities, allowing for insightful comparisons across biological repeats and conditions."],"metadata":{"id":"DSinIcHH3YT9"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JrkfFr7mgZmA"},"outputs":[],"source":["# @title #MIT License\n","\n","print(\"\"\"\n","**MIT License**\n","\n","Copyright (c) 2023 Guillaume Jacquemet\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy\n","of this software and associated documentation files (the \"Software\"), to deal\n","in the Software without restriction, including without limitation the rights\n","to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n","copies of the Software, and to permit persons to whom the Software is\n","furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all\n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n","AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n","OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n","SOFTWARE.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"Y4-Ft-yNRVCc"},"source":["--------------------------------------------------------\n","# **Part 1. Prepare the session and load your data**\n","--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"9h0prdayn0qG"},"source":["## **1.1. Install key dependencies**\n","---\n","<font size = 4>"]},{"cell_type":"code","source":["#@markdown ##Play to install\n","!pip -q install pandas scikit-learn\n","!pip -q install hdbscan\n","!pip -q install umap-learn\n","!pip -q install plotly\n","!pip -q install prettytable\n","!pip -q install adjustText\n","\n","\n","\n"],"metadata":{"cellView":"form","id":"S_BZuYOQGo1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAP0ahCzn1V6","cellView":"form"},"outputs":[],"source":["#@markdown ##Play to load the dependancies\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import numpy as np\n","import itertools\n","from matplotlib.gridspec import GridSpec\n","import requests\n","\n","!pip freeze > requirements.txt\n","\n","# Current version of the notebook the user is running\n","current_version = \"0.1\"\n","Notebook_name = 'dimensionality_reduction'\n","\n","# URL to the raw content of the version file in the repository\n","version_url = \"https://raw.githubusercontent.com/CellMigrationLab/Plot-Stats/main/Notebooks/latest_version.txt\"\n","\n","# Function to define colors for formatting messages\n","class bcolors:\n","    WARNING = '\\033[91m'  # Red color for warning messages\n","    ENDC = '\\033[0m'      # Reset color to default\n","\n","# Check if this is the latest version of the notebook\n","try:\n","    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n","    print('Notebook version: ' + current_version)\n","\n","    # Check if 'Version' column exists in the DataFrame\n","    if 'Version' in All_notebook_versions.columns:\n","        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n","        print('Latest notebook version: ' + Latest_Notebook_version)\n","\n","        if current_version == Latest_Notebook_version:\n","            print(\"This notebook is up-to-date.\")\n","        else:\n","            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/CellMigrationLab/Plot-Stats\" + bcolors.ENDC)\n","    else:\n","        print(\"The 'Version' column is not present in the version file.\")\n","except requests.exceptions.RequestException as e:\n","    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n","except Exception as e:\n","    print(\"An error occurred:\", str(e))\n","\n","\n","\n","\n","# Function to calculate Cohen's d\n","def cohen_d(group1, group2):\n","    diff = group1.mean() - group2.mean()\n","    n1, n2 = len(group1), len(group2)\n","    var1 = group1.var()\n","    var2 = group2.var()\n","    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n","    d = diff / np.sqrt(pooled_var)\n","    return d\n","\n","def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n","    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n","\n","    # Estimating the number of chunks based on the provided chunk size\n","    num_chunks = int(len(df) / chunk_size) + 1\n","\n","    # Create a tqdm instance for progress tracking\n","    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n","        # Open the file for writing\n","        with open(path, \"w\") as f:\n","            # Write the header once at the beginning\n","            df.head(0).to_csv(f, index=False)\n","\n","            for chunk in np.array_split(df, num_chunks):\n","                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n","                pbar.update(len(chunk))\n","\n","def check_for_nans(df, df_name):\n","    \"\"\"\n","    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n","\n","    Args:\n","    df (pd.DataFrame): DataFrame to be checked for NaN values.\n","    df_name (str): The name of the DataFrame as a string, used for printing.\n","    \"\"\"\n","    # Check if the DataFrame has any NaN values and print a warning if it does.\n","    nan_columns = df.columns[df.isna().any()].tolist()\n","\n","    if nan_columns:\n","        for col in nan_columns:\n","            nan_count = df[col].isna().sum()\n","            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n","    else:\n","        print(f\"No NaN values found in {df_name}.\")\n","\n","\n","import pandas as pd\n","import os\n","\n","def save_parameters(params, file_path, param_type):\n","    # Convert params dictionary to a DataFrame for human readability\n","    new_params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])\n","    new_params_df['Type'] = param_type\n","\n","    if os.path.exists(file_path):\n","        # Read existing file\n","        existing_params_df = pd.read_csv(file_path)\n","\n","        # Merge the new parameters with the existing ones\n","        # Update existing parameters or append new ones\n","        updated_params_df = pd.merge(existing_params_df, new_params_df,\n","                                     on=['Type', 'Parameter'],\n","                                     how='outer',\n","                                     suffixes=('', '_new'))\n","\n","        # If there's a new value, update it, otherwise keep the old value\n","        updated_params_df['Value'] = updated_params_df['Value_new'].combine_first(updated_params_df['Value'])\n","\n","        # Drop the temporary new value column\n","        updated_params_df.drop(columns='Value_new', inplace=True)\n","    else:\n","        # Use new parameters DataFrame directly if file doesn't exist\n","        updated_params_df = new_params_df\n","\n","    # Save the updated DataFrame to CSV\n","    updated_params_df.to_csv(file_path, index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"3Kzd_8GUnpbw"},"source":["## **1.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GA1wCrkoV4i5"},"outputs":[],"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bsDAwkSOo1gV"},"source":["## **1.3. Load your dataset**\n","---\n","\n","<font size = 4> Please ensure that your data is properly organised (see above)\n"]},{"cell_type":"code","source":["#@markdown ##Load your dataset:\n","\n","\n","import pandas as pd\n","import os\n","from io import StringIO\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","\n","# Initialize dataset_df as an empty DataFrame globally\n","dataset_df = pd.DataFrame()\n","\n","\n","# Create widgets\n","dataset_path_input = widgets.Text(\n","    value='',\n","    placeholder='Enter the path to your dataset',\n","    description='Dataset Path:',\n","    layout={'width': '80%'}\n",")\n","\n","results_folder_input = widgets.Text(\n","    value='',\n","    placeholder='Enter the path to your results folder',\n","    description='Results Folder:',\n","    layout={'width': '80%'}\n",")\n","\n","data_textarea = widgets.Textarea(\n","    value='',\n","    placeholder='Or copy and paste your tab sperated data here (direct copy and paste from a spreedsheet)',\n","    description='Or Paste Data:',\n","    layout={'width': '80%', 'height': '200px'}\n",")\n","\n","load_button = widgets.Button(\n","    description='Load Data',\n","    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n","    tooltip='Click to load the data',\n",")\n","\n","output = widgets.Output()\n","\n","# Load data function\n","def load_data(b):\n","    global dataset_df\n","    global Results_Folder\n","\n","    with output:\n","        clear_output()\n","        Results_Folder = results_folder_input.value.strip()\n","        if not Results_Folder:\n","            Results_Folder = './Results'  # Default path if not provided\n","        if not os.path.exists(Results_Folder):\n","            os.makedirs(Results_Folder)  # Create the folder if it doesn't exist\n","        print(f\"Results folder is located at: {Results_Folder}\")\n","\n","        if dataset_path_input.value.strip():\n","            dataset_path = dataset_path_input.value.strip()\n","            try:\n","                dataset_df = pd.read_csv(dataset_path)\n","                print(f\"Loaded dataset from {dataset_path}\")\n","            except Exception as e:\n","                print(f\"Failed to load dataset from {dataset_path}: {e}\")\n","        elif data_textarea.value.strip():\n","            input_data = StringIO(data_textarea.value)\n","            try:\n","                dataset_df = pd.read_csv(input_data, sep='\\t')\n","                print(\"Loaded dataset from pasted tab-separated data\")\n","            except Exception as e:\n","                print(f\"Failed to load dataset from pasted data: {e}\")\n","        else:\n","            print(\"No dataset path provided or data pasted. Please provide a dataset.\")\n","            return\n","\n","        # Perform a check for NaNs or any other required processing here\n","        check_for_nans(dataset_df, \"your dataset\")\n","\n","        display(dataset_df.head())\n","\n","# Set the button click event\n","load_button.on_click(load_data)\n","\n","# Display the widgets\n","display(widgets.VBox([dataset_path_input, results_folder_input, data_textarea, load_button, output]))\n"],"metadata":{"cellView":"form","id":"yIX1uUc3NpCS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **1.4. Map your data**\n","---\n","\n","## Required Columns\n","\n","<font size = 4>To plot your data, we need to ensure the presence of specific columns in the dataset. Here's a breakdown of the required columns:\n","\n","- **`Condition`**: Identifies the biological condition.\n","\n","- **`Repeat`**: Represents the biological repeat.\n"],"metadata":{"id":"dr9Wm3BIHnuI"}},{"cell_type":"code","source":["#@markdown ##Map your dataset:\n","\n","\n","import ipywidgets as widgets  # Ensure we have the required widgets module imported\n","import pandas as pd\n","\n","def single_stage_column_mapping(df):\n","    # Define the columns we need to map: Condition, Repeat\n","    mappings = {\n","        'Condition': 'Identifies the biological conditions.',\n","        'Repeat': 'Represents the biological repeats.'\n","    }\n","\n","    dropdowns = {}\n","    for key, description in mappings.items():\n","        description_label = widgets.Label(f\"{key} ({description}):\")\n","        dropdowns[key] = widgets.Dropdown(options=df.columns, layout=widgets.Layout(width='250px'))\n","\n","        # Use HBox to display the description label next to the dropdown\n","        hbox = widgets.HBox([description_label, dropdowns[key]])\n","        display(hbox)\n","\n","    confirm_button = widgets.Button(description=\"Confirm Mappings\")\n","\n","    def confirm_mappings(button):\n","        # Perform the mapping based on the user selection\n","        column_mapping = {dropdown.value: key for key, dropdown in dropdowns.items()}\n","        new_df = df.rename(columns=column_mapping)\n","\n","        print(\"Columns Mapped Successfully!\")\n","\n","        # Count and print unique conditions\n","        unique_conditions = new_df['Condition'].unique()\n","        print(f\"Number of unique conditions: {len(unique_conditions)}\")\n","        print(\"Conditions:\", \", \".join(unique_conditions))\n","\n","        # Count and print biological repeats\n","        unique_repeats = new_df['Repeat'].unique()\n","        print(f\"Number of biological repeats: {len(unique_repeats)}\")\n","        print(\"Repeats:\", \", \".join(map(str, unique_repeats)))\n","\n","\n","        # Check that each biological condition has exactly the same repeat names\n","        condition_repeats = new_df.groupby('Condition')['Repeat'].apply(set)\n","        if len(set(map(frozenset, condition_repeats))) == 1:\n","            print(\"All biological conditions have exactly the same repeat names.\")\n","        else:\n","            print(\"Warning: Not all biological conditions have the same repeat names.\")\n","\n","        # Update the global dataset_df with the new mappings\n","        global dataset_df\n","        dataset_df = new_df\n","\n","    confirm_button.on_click(confirm_mappings)\n","    display(confirm_button)\n","\n","single_stage_column_mapping(dataset_df)\n"],"metadata":{"id":"5VE9oqIEHrG-","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["--------\n","# **Part 2. Explore your high-dimensional data using PCA and HDBSCAN**\n","--------"],"metadata":{"id":"6856SOTl0LN8"}},{"cell_type":"markdown","metadata":{"id":"h78S7ZwF0T9C"},"source":["## **2.1. Choose the Conditions to use**\n","\n"]},{"cell_type":"code","source":["#@markdown ##Choose the conditions to use:\n","\n","import pandas as pd\n","import ipywidgets as widgets\n","from IPython.display import display\n","import os\n","\n","PCA_folder_path = os.path.join(Results_Folder, \"PCA\")\n","if not os.path.exists(PCA_folder_path):\n","    os.makedirs(PCA_folder_path)\n","\n","# Function to parse the text area content into a list\n","def parse_text_area(text):\n","    return [item.strip() for item in text.split(',') if item.strip()]\n","\n","# Text area for user to paste the list of conditions\n","text_area_conditions = widgets.Textarea(\n","    value='',\n","    placeholder='Copy and paste your list of conditions here, separated by commas. Or tick the boxes below.',\n","    description='Conditions:',\n","    disabled=False,\n","    layout=widgets.Layout(width='80%', height='100px')\n",")\n","\n","# Create checkboxes for each unique condition in the dataset\n","Condition_checkboxes = [widgets.Checkbox(value=True, description=str(condition)) for condition in dataset_df['Condition'].unique()]\n","\n","# Function to filter dataframe based on selected checkbox values and text area input\n","def filter_dataframe(button):\n","    global filtered_df\n","    # Initialize an empty list to hold selected conditions\n","    selected_conditions = []\n","\n","    # Check if the text area is not empty\n","    if text_area_conditions.value.strip():\n","        # Use conditions from the text area\n","        selected_conditions = parse_text_area(text_area_conditions.value)\n","    else:\n","        # Use conditions from checkboxes if the text area is empty\n","        selected_conditions = [box.description for box in Condition_checkboxes if box.value]\n","\n","    # Filter DataFrame\n","    filtered_df = dataset_df[dataset_df['Condition'].isin(selected_conditions)]\n","\n","    print(\"Selected Conditions:\", selected_conditions)\n","    print(\"Filtered DataFrame length:\", len(filtered_df))\n","    if len(filtered_df) == 0:\n","        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n","\n","    # Save selected conditions as parameters\n","    params = {'Selected Conditions': ', '.join(selected_conditions)}\n","    params_file_path = os.path.join(PCA_folder_path, \"PCA_analysis_parameters.csv\")\n","    save_parameters(params, params_file_path, 'PCA Conditions')\n","\n","# Button to trigger dataframe filtering\n","filter_button = widgets.Button(description=\"Filter Dataframe\")\n","filter_button.on_click(filter_dataframe)\n","\n","# Display text area, checkboxes, and button\n","display(text_area_conditions, widgets.VBox([widgets.Label('Select Conditions:')] + Condition_checkboxes + [filter_button]))\n"],"metadata":{"cellView":"form","id":"bERbWDkw5Cjk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2.2. Choose the numeric data to use**\n","\n"],"metadata":{"id":"bpI15DyWXZh3"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","#@markdown ##Choose the numeric data to use\n","\n","\n","PCA_folder_path = os.path.join(Results_Folder, \"PCA\")\n","if not os.path.exists(PCA_folder_path):\n","    os.makedirs(PCA_folder_path)\n","\n","# Define columns to preserve and to exclude if present\n","preserve_columns = ['Condition']\n","exclude_columns = ['Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","\n","# Ensure 'Condition' column and potentially 'Repeat' column are handled in 'numeric_df' for later use\n","numeric_columns = filtered_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n","# Keep a copy of the 'Condition' column along with numeric columns\n","# Exclude 'Repeat' column if it exists\n","numeric_df = filtered_df[numeric_columns + preserve_columns].copy()\n","if 'Repeat' in numeric_df.columns:\n","    numeric_df = numeric_df.drop(columns=['Repeat'])\n","\n","# Text area for user to paste the list of metrics\n","text_area = widgets.Textarea(\n","    value='',\n","    placeholder='Copy and paste your list of metrics here, separated by commas. Or tick the boxes below.',\n","    description='Metrics:',\n","    disabled=False,\n","    layout=widgets.Layout(width='80%', height='100px')\n",")\n","\n","# Function to parse the text area content into a list\n","def parse_text_area(text):\n","    return [item.strip() for item in text.split(',') if item.strip() in numeric_columns]\n","\n","# Create a checkbox for each numeric column (excluding 'Condition' and potentially 'Repeat')\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in numeric_columns if col not in exclude_columns]\n","\n","# Arrange checkboxes in a grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection\n","button = widgets.Button(description=\"Select Track Parameters\", layout=widgets.Layout(width='400px'))\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df\n","\n","    # Use metrics from the text area if not empty, else from checkboxes\n","    if text_area.value.strip():\n","        selected_columns = parse_text_area(text_area.value)\n","    else:\n","        selected_columns = [box.description for box in checkboxes if box.value]\n","\n","    selected_df = numeric_df[selected_columns + preserve_columns].copy()\n","\n","    # Check for NaN values and handle them\n","    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n","    if nan_columns:\n","        for col in nan_columns:\n","            selected_df.dropna(subset=[col], inplace=True)  # Drop rows with NaN values in these columns\n","            print(f\"Dropped rows from column '{col}' due to NaN values.\")\n","\n","    print(\"Track parameters selected and NaN values handled.\")\n","    params = {'Selected numeric data': ', '.join(selected_columns)}\n","    params_file_path = os.path.join(PCA_folder_path, \"PCA_analysis_parameters.csv\")\n","    save_parameters(params, params_file_path, 'PCA numeric data')\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the text area, grid of checkboxes, and the button\n","display(text_area, grid, button)\n"],"metadata":{"cellView":"form","id":"ftUa73Em5x4K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2.3. Perform the PCA**\n","\n"],"metadata":{"id":"n4PC3HOE0YE9"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import warnings\n","import pandas as pd\n","import plotly.express as px\n","\n","\n","# Verify 'Condition' column exists\n","if 'Condition' not in filtered_df.columns:\n","    raise ValueError(\"The 'Condition' column is missing from the DataFrame.\")\n","\n","# Check and create \"PCA\" folder\n","pca_folder_path = f\"{Results_Folder}/PCA\"\n","if not os.path.exists(pca_folder_path):\n","    os.makedirs(pca_folder_path)\n","\n","#@markdown ###PCA parameters:\n","n_dimension = 2  # @param {type: \"slider\", min: 1, max: 3}\n","\n","#@markdown ###Display parameters:\n","spot_size = 10 # @param {type: \"number\"}\n","\n","# Assuming 'numeric_data' contains only numeric columns and excludes columns like 'Condition'\n","# If 'selected_df' already only contains numeric data and 'Condition', adjust accordingly\n","numeric_columns = selected_df.select_dtypes(include=['float64', 'int64']).columns\n","numeric_data = selected_df[numeric_columns]\n","\n","# Initialize and fit StandardScaler on the data\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(numeric_data)\n","\n","# Initialize PCA object with the specified settings\n","pca = PCA(n_components=n_dimension, random_state=42)\n","# Fit PCA on the scaled data\n","pca_result = pca.fit_transform(scaled_data)\n","\n","# Create dynamic column names based on n_components\n","column_names = [f'PCA Dimension {i}' for i in range(1, n_dimension + 1)]\n","\n","# Combine PCA results with 'Condition' column\n","pca_df = pd.DataFrame(pca_result, columns=column_names)\n","pca_df['Condition'] = filtered_df['Condition'].values  # Directly assign 'Condition' values to ensure alignment\n","\n","\n","# Check for NaN values in the resulting DataFrame\n","nan_columns = pca_df.columns[pca_df.isna().any()].tolist()\n","if nan_columns:\n","    warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n","    pca_df.dropna(subset=nan_columns, inplace=True)\n","\n","# Visualization\n","plt.figure(figsize=(12, 10))\n","sns.set(style=\"whitegrid\")\n","\n","if n_dimension == 2:\n","    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=pca_df, palette='Set2', s=spot_size)\n","    plt.title('PCA Projection to 2D')\n","    plt.savefig(f\"{pca_folder_path}/PCA_projection_2D.pdf\")\n","elif n_dimension == 1:\n","    sns.stripplot(x=column_names[0], hue='Condition', data=pca_df, palette='Set2', jitter=True, size=spot_size)\n","    plt.title('PCA Projection to 1D')\n","    plt.savefig(f\"{pca_folder_path}/PCA_projection_1D.pdf\")\n","else:  # For 3D plots\n","# Using Plotly Express to create a 3D scatter plot\n","    fig = px.scatter_3d(pca_df, x='PCA Dimension 1', y='PCA Dimension 2', z='PCA Dimension 3',\n","                    color='Condition', title='3D PCA Projection', opacity=0.7)\n","\n","# Optional: Customize the layout\n","    fig.update_layout(margin=dict(l=0, r=0, b=0, t=30))\n","\n","# Show the plot\n","    fig.show()\n","\n","# Save the plot as an HTML file\n","    html_file_path = os.path.join(Results_Folder, \"PCA/PCA_3d_projection.html\")\n","    fig.write_html(html_file_path)\n","    print(f\"3D PCA plot saved to: {html_file_path}\")\n","\n","    plt.show()\n","\n","\n","# Save parameters used in PCA\n","PCA_params = {\n","    'n_dimension': n_dimension,\n","    'spot_size': spot_size\n","}\n","params_file_path = os.path.join(pca_folder_path, \"PCA_analysis_parameters.csv\")\n","save_parameters(PCA_params, params_file_path, 'PCA')\n","\n"],"metadata":{"cellView":"form","id":"8PkRj4DX6HKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2.4. Display the PCA Loadings**\n","\n"],"metadata":{"id":"uraBqCmaX2Jo"}},{"cell_type":"code","source":["#@markdown ##Display the PCA Loadings\n","\n","from adjustText import adjust_text\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import os\n","\n","# Ensure the Results_Folder/PCA directory exists\n","save_path = os.path.join(Results_Folder, \"PCA\")\n","os.makedirs(save_path, exist_ok=True)\n","\n","# Loadings calculation\n","loadings = pca.components_.T\n","\n","if n_dimension == 1:\n","    plt.figure(figsize=(12, 10))\n","    sns.scatterplot(x='PCA Dimension 1', y=[0] * len(pca_df), hue='Condition', data=pca_df, palette='Set2', s=10, alpha=0.7)\n","    plt.xlabel('PCA Dimension 1')\n","    plt.title('PCA Projection to 1D with Loadings')\n","    # Loadings are not typically visualized in 1D PCA plots.\n","\n","elif n_dimension == 2:\n","    plt.figure(figsize=(12, 10))\n","    sns.scatterplot(x='PCA Dimension 1', y='PCA Dimension 2', hue='Condition', data=pca_df, palette='Set2', s=10, alpha=0.7)\n","    scale_factor = max(pca_df['PCA Dimension 1'].max(), pca_df['PCA Dimension 2'].max()) / max(loadings[:, 0].max(), loadings[:, 1].max())\n","    texts = []\n","    for i, feature in enumerate(numeric_data.columns):\n","        plt.arrow(0, 0, loadings[i, 0] * scale_factor, loadings[i, 1] * scale_factor, color='r', alpha=0.5)\n","        texts.append(plt.text(loadings[i, 0] * scale_factor * 1.1, loadings[i, 1] * scale_factor * 1.1, feature, color='g', ha='center', va='center'))\n","    adjust_text(texts, arrowprops=dict(arrowstyle='->', color='gray'))\n","    plt.xlabel('PCA Dimension 1')\n","    plt.ylabel('PCA Dimension 2')\n","    plt.title('PCA Projection to 2D with Loadings')\n","    plt.grid(True)\n","\n","elif n_dimension == 3:\n","    print(\"Feature not available for 3D yet.\")\n","\n","# Save the figure for 1D and 2D cases\n","if n_dimension in [1, 2]:\n","    plt_path = os.path.join(save_path, f'PCA_projection_with_loadings_{n_dimension}D.pdf')\n","    plt.savefig(plt_path, bbox_inches='tight')\n","    plt.show()\n","    print(f\"Plot saved to: {plt_path}\")\n"],"metadata":{"cellView":"form","id":"exfDGjQ8_Hdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title ##Print the PCA loading vectors\n","\n","\n","# Now, create the DataFrame for loadings\n","loadings = pd.DataFrame(pca.components_.T,  # Transpose the components matrix\n","                        columns=[f'PCA Dimension {i}' for i in range(1, n_dimension + 1)],  # Naming columns based on the number of dimensions\n","                        index=numeric_columns)  # Using the numeric columns as the index\n","\n","# Display the loadings DataFrame\n","print(loadings)\n","\n","# Specify the path for the CSV file where you want to save the loadings\n","loadings_file_path = os.path.join(Results_Folder, \"PCA\", \"PCA_loadings_vectors.csv\")\n","\n","# Save the loadings DataFrame to a CSV file\n","loadings.to_csv(loadings_file_path)\n","\n","print(f\"PCA loadings vectors saved to: {loadings_file_path}\")\n","\n"],"metadata":{"cellView":"form","id":"y1bB_QBN-CJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPWiX6R236jX"},"source":["-------------------------------------------\n","\n","# **Part 3. Explore your high-dimensional data using UMAP and HDBSCAN**\n","-------------------------------------------\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fvXoSq5wZDPb"},"source":["## **3.1. Choose the Conditions to use**\n","\n"]},{"cell_type":"code","source":["#@markdown ##Choose the conditions to use:\n","\n","import pandas as pd\n","import ipywidgets as widgets\n","from IPython.display import display\n","import os\n","\n","UMAP_folder_path = os.path.join(Results_Folder, \"UMAP\")\n","if not os.path.exists(UMAP_folder_path):\n","    os.makedirs(UMAP_folder_path)\n","\n","# Function to parse the text area content into a list\n","def parse_text_area(text):\n","    return [item.strip() for item in text.split(',') if item.strip()]\n","\n","# Text area for user to paste the list of conditions\n","text_area_conditions = widgets.Textarea(\n","    value='',\n","    placeholder='Copy and paste your list of conditions here, separated by commas. Or tick the boxes below.',\n","    description='Conditions:',\n","    disabled=False,\n","    layout=widgets.Layout(width='80%', height='100px')\n",")\n","\n","# Create checkboxes for each unique condition in the dataset\n","Condition_checkboxes = [widgets.Checkbox(value=True, description=str(condition)) for condition in dataset_df['Condition'].unique()]\n","\n","# Function to filter dataframe based on selected checkbox values and text area input\n","def filter_dataframe(button):\n","    global filtered_df\n","    # Initialize an empty list to hold selected conditions\n","    selected_conditions = []\n","\n","    # Check if the text area is not empty\n","    if text_area_conditions.value.strip():\n","        # Use conditions from the text area\n","        selected_conditions = parse_text_area(text_area_conditions.value)\n","    else:\n","        # Use conditions from checkboxes if the text area is empty\n","        selected_conditions = [box.description for box in Condition_checkboxes if box.value]\n","\n","    # Filter DataFrame\n","    filtered_df = dataset_df[dataset_df['Condition'].isin(selected_conditions)]\n","\n","    print(\"Selected Conditions:\", selected_conditions)\n","    print(\"Filtered DataFrame length:\", len(filtered_df))\n","    if len(filtered_df) == 0:\n","        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n","\n","    # Save selected conditions as parameters\n","    params = {'Selected Conditions': ', '.join(selected_conditions)}\n","    params_file_path = os.path.join(UMAP_folder_path, \"UMAP_analysis_parameters.csv\")\n","    save_parameters(params, params_file_path, 'UMAP Conditions')\n","\n","# Button to trigger dataframe filtering\n","filter_button = widgets.Button(description=\"Filter Dataframe\")\n","filter_button.on_click(filter_dataframe)\n","\n","# Display text area, checkboxes, and button\n","display(text_area_conditions, widgets.VBox([widgets.Label('Select Conditions:')] + Condition_checkboxes + [filter_button]))\n"],"metadata":{"cellView":"form","id":"iOQDMprR8RmI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3.2. Choose the numeric data to use**\n","\n"],"metadata":{"id":"CEhEB718Y_sH"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","#@markdown ##Choose the numeric data to use\n","\n","\n","UMAP_folder_path = os.path.join(Results_Folder, \"UMAP\")\n","if not os.path.exists(UMAP_folder_path):\n","    os.makedirs(UMAP_folder_path)\n","\n","# Define columns to preserve and to exclude if present\n","preserve_columns = ['Condition']\n","exclude_columns = ['Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","\n","# Ensure 'Condition' column and potentially 'Repeat' column are handled in 'numeric_df' for later use\n","numeric_columns = filtered_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n","# Keep a copy of the 'Condition' column along with numeric columns\n","# Exclude 'Repeat' column if it exists\n","numeric_df = filtered_df[numeric_columns + preserve_columns].copy()\n","if 'Repeat' in numeric_df.columns:\n","    numeric_df = numeric_df.drop(columns=['Repeat'])\n","\n","# Text area for user to paste the list of metrics\n","text_area = widgets.Textarea(\n","    value='',\n","    placeholder='Copy and paste your list of metrics here, separated by commas. Or tick the boxes below',\n","    description='Metrics:',\n","    disabled=False,\n","    layout=widgets.Layout(width='80%', height='100px')\n",")\n","\n","# Function to parse the text area content into a list\n","def parse_text_area(text):\n","    return [item.strip() for item in text.split(',') if item.strip() in numeric_columns]\n","\n","# Create a checkbox for each numeric column (excluding 'Condition' and potentially 'Repeat')\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in numeric_columns if col not in exclude_columns]\n","\n","# Arrange checkboxes in a grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection\n","button = widgets.Button(description=\"Select Track Parameters\", layout=widgets.Layout(width='400px'))\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df\n","\n","    # Use metrics from the text area if not empty, else from checkboxes\n","    if text_area.value.strip():\n","        selected_columns = parse_text_area(text_area.value)\n","    else:\n","        selected_columns = [box.description for box in checkboxes if box.value]\n","\n","    selected_df = numeric_df[selected_columns + preserve_columns].copy()\n","\n","    # Check for NaN values and handle them\n","    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n","    if nan_columns:\n","        for col in nan_columns:\n","            selected_df.dropna(subset=[col], inplace=True)  # Drop rows with NaN values in these columns\n","            print(f\"Dropped rows from column '{col}' due to NaN values.\")\n","\n","    print(\"Track parameters selected and NaN values handled.\")\n","    params = {'Selected numeric data': ', '.join(selected_columns)}\n","    params_file_path = os.path.join(UMAP_folder_path, \"UMAP_analysis_parameters.csv\")\n","    save_parameters(params, params_file_path, 'UMAP numeric data')\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the text area, grid of checkboxes, and the button\n","display(text_area, grid, button)\n"],"metadata":{"cellView":"form","id":"S3sSSR_38RmI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLlDuCKmlxvl"},"source":["## **3.3. UMAP**\n","---\n","\n","<font size = 4> The given code performs UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the merged tracks dataframe, focusing on its numeric columns, and visualizes the result. In the provided UMAP code, the parameters `n_neighbors`, `min_dist`, and `n_components` are crucial for determining the structure and appearance of the resulting low-dimensional representation of the data.\n","\n","<font size = 4>`n_neighbors`: This parameter controls how UMAP balances local versus global structure in the data. It determines the size of the local neighborhood UMAP will look at when learning the manifold structure of the data.\n","- A smaller value emphasizes the local structure of the data, potentially at the expense of the global structure.\n","- A larger value allows UMAP to consider more distant neighbors, emphasizing more on the global structure of the data.\n","- Typically, values in the range of 5 to 50 are chosen, depending on the density and scale of the data.\n","\n","<font size = 4>`min_dist`: This parameter controls how tightly UMAP is allowed to pack points together. It determines the minimum distance between points in the low-dimensional representation.\n","- Setting it to a low value will allow points to be packed more closely, potentially revealing clusters in the data.\n","- A higher value ensures that points are more spread out in the representation.\n","- Values usually range between 0 and 1.\n","\n","<font size = 4>`n_dimension`: This parameter determines the number of dimensions in the low-dimensional space that the data will be reduced to.\n","For visualization purposes, `n_dimension` is typically set to 2 or 3 to obtain 2D or 3D representations, respectively.\n"]},{"cell_type":"code","source":["import umap\n","import plotly.offline as pyo\n","import plotly.express as px\n","import pandas as pd\n","import os\n","import warnings\n","\n","#@markdown ###UMAP parameters:\n","\n","n_neighbors = 10  # @param {type: \"number\"}\n","min_dist = 0  # @param {type: \"number\"}\n","n_dimension = 2  # @param {type: \"slider\", min: 1, max: 3}\n","\n","#@markdown ###Display parameters:\n","spot_size = 15 # @param {type: \"number\"}\n","\n","# Assuming 'selected_df' is correctly defined and includes 'Condition' column\n","if 'Condition' not in selected_df.columns:\n","    raise KeyError(\"The 'Condition' column is missing from the selected_df DataFrame.\")\n","\n","\n","# Initialize UMAP object with the specified settings\n","reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_dimension, random_state=42)\n","\n","# Assuming 'numeric_columns' contains the names of numeric columns used for UMAP\n","numeric_columns = selected_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n","numeric_data = selected_df[numeric_columns]\n","\n","# Fit UMAP and transform the data\n","embedding = reducer.fit_transform(numeric_data)\n","\n","# Create dynamic column names based on n_components\n","column_names = [f'UMAP Dimension {i}' for i in range(1, n_dimension + 1)]\n","\n","# Concatenate the UMAP embedding with the 'Condition' column\n","umap_df = pd.concat([pd.DataFrame(embedding, columns=column_names), selected_df[['Condition']].reset_index(drop=True)], axis=1)\n","\n","# Prepare the parameters dictionary for saving\n","UMAP_params = {'n_neighbors': n_neighbors, 'min_dist': min_dist, 'n_dimension': n_dimension}\n","\n","# Save the parameters\n","params_file_path = os.path.join(Results_Folder, \"UMAP/UMAP_analysis_parameters.csv\")\n","\n","save_parameters(UMAP_params, params_file_path, 'UMAP parameters')\n","\n","# Visualize the UMAP projection\n","if n_dimension in [1, 2]:\n","    plt.figure(figsize=(12, 10))\n","    sns.scatterplot(x=column_names[0], y=column_names[1] if n_dimension == 2 else [0]*len(umap_df), hue='Condition', data=umap_df, palette='Set2', s=spot_size)\n","    plt.title(f'UMAP Projection to {n_dimension}D')\n","    plt.savefig(os.path.join(Results_Folder, f\"UMAP/UMAP_projection_{n_dimension}D.pdf\"))\n","    plt.show()\n","elif n_dimension == 3:\n","    fig = px.scatter_3d(umap_df, x='UMAP Dimension 1', y='UMAP Dimension 2', z='UMAP Dimension 3', color='Condition', size_max=spot_size)\n","    fig.update_traces(marker=dict(size=spot_size))\n","    fig.show()\n","    html_file_path = os.path.join(Results_Folder, \"UMAP/umap_projection_3D.html\")\n","    pyo.plot(fig, filename=html_file_path, auto_open=False)\n","    print(f\"UMAP 3D projection saved to: {html_file_path}\")\n","else:\n","    warnings.warn(\"Invalid number of dimensions for UMAP projection.\")\n"],"metadata":{"cellView":"form","id":"DbkY465sfPE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hMx8DVRMmvKQ"},"source":["## **3.4. HDBSCAN**\n","---\n","\n","<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n","\n","<font size = 4>In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n","\n","<font size = 4>`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n","- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n","- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n","- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n","\n","<font size = 4>`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n","- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n","- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n","\n","<font size = 4>`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n","- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PGdYdL7hnrpK"},"outputs":[],"source":["# @title ##Run to see more information about the available metrics\n","print(\"\"\"\n","Metric                   Description                                                               Suitable For\n","-------------------------------------------------------------------------------------------------------------------------------------------------------\n","Euclidean                Standard distance metric.                                                 Numerical data.\n","Manhattan                Sum of absolute differences.                                              Numerical/Categorical data.\n","Chebyshev                Maximum value of absolute differences.                                    Numerical data.\n","Bray-Curtis              Dissimilarity between sample sets.                                        Numerical data.\n","Canberra                 Weighted version of Manhattan distance.                                   Numerical data.\n","\n","\"\"\")\n"]},{"cell_type":"code","source":["import hdbscan\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","# @title ##Identify clusters using HDBSCAN\n","\n","#@markdown ###HDBSCAN parameters:\n","clustering_data_source = 'umap'  # @param ['umap', 'raw']\n","min_samples = 10  # @param {type: \"number\"}\n","min_cluster_size = 100  # @param {type: \"number\"}\n","metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n","\n","#@markdown ###Display parameters:\n","spot_size = 20 # @param {type: \"number\"}\n","\n","umap_folder_path = os.path.join(Results_Folder, \"UMAP\")\n","os.makedirs(umap_folder_path, exist_ok=True)\n","\n","# Initialize HDBSCAN with specified settings\n","clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)\n","\n","# Fit HDBSCAN based on the specified data source\n","if clustering_data_source == 'umap':\n","    # Construct a list of UMAP dimension column names based on n_dimension\n","    umap_columns = [f'UMAP Dimension {i}' for i in range(1, n_dimension + 1)]\n","    data_for_clustering = umap_df[umap_columns]\n","else:\n","    # Use selected numeric data for clustering if 'raw' data source is chosen\n","    data_for_clustering = selected_df.select_dtypes(include=[np.number])\n","\n","# Apply clustering\n","clusterer.fit(data_for_clustering)\n","\n","# Add the cluster labels to umap_df\n","umap_df['Cluster_UMAP'] = clusterer.labels_\n","\n","# Plotting the results based on n_dimension\n","plot_title = 'Clusters Identified by HDBSCAN'\n","if n_dimension == 1:\n","    plt.figure(figsize=(12, 6))\n","    sns.stripplot(x='UMAP Dimension 1', hue='Cluster_UMAP', data=umap_df, palette='viridis', size=spot_size)\n","    plt.title(f'{plot_title} (1D)')\n","elif n_dimension == 2:\n","    plt.figure(figsize=(12, 10))\n","    sns.scatterplot(x='UMAP Dimension 1', y='UMAP Dimension 2', hue='Cluster_UMAP', data=umap_df, palette='viridis', size=spot_size)\n","    plt.title(f'{plot_title} (2D)')\n","elif n_dimension == 3:\n","    fig = px.scatter_3d(umap_df, x='UMAP Dimension 1', y='UMAP Dimension 2', z='UMAP Dimension 3', color='Cluster_UMAP', size_max=spot_size)\n","    fig.update_traces(marker=dict(size=spot_size/10))\n","    fig.show()\n","    # Save the 3D plot as HTML\n","    html_file_path = os.path.join(umap_folder_path, \"HDBSCAN_clusters_3D.html\")\n","    fig.write_html(html_file_path, auto_open=False)\n","    print(f\"3D cluster plot saved to: {html_file_path}\")\n","else:\n","    print(\"Invalid n_dimension value for clustering visualization.\")\n","\n","\n","# Save the plots as PDF for 1D and 2D visualizations\n","if n_dimension in [1, 2]:\n","    pdf_file_path = os.path.join(umap_folder_path, f\"HDBSCAN_clusters_{n_dimension}D.pdf\")\n","    plt.savefig(pdf_file_path)\n","    plt.show()\n","    print(f\"{n_dimension}D cluster plot saved to: {pdf_file_path}\")\n","\n","dataset_df.reset_index(drop=True, inplace=True)\n","umap_df.reset_index(drop=True, inplace=True)\n","\n","# If the Cluster column already exists in dataset_df, drop it to avoid duplications\n","if 'Cluster_UMAP' in dataset_df.columns:\n","    dataset_df.drop(columns='Cluster_UMAP', inplace=True)\n","\n","# Directly join the 'Cluster_UMAP' column to dataset_df based on index\n","dataset_df['Cluster_UMAP'] = umap_df['Cluster_UMAP']\n","\n","# Optionally, fill NaN values with -1 if there are unassigned clusters\n","dataset_df['Cluster_UMAP'].fillna(-1, inplace=True)\n","\n","# Save the DataFrame with the identified clusters\n","dataset_df.to_csv(f\"{Results_Folder}/merged_dataset_clusters.csv\", index=False)\n","\n","\n","# Save clustering parameters and results\n","params_file_path = os.path.join(umap_folder_path, \"UMAP_analysis_parameters.csv\")\n","HDBSCAN_params = {\n","    'clustering_data_source': clustering_data_source,\n","    'min_samples': min_samples,\n","    'min_cluster_size': min_cluster_size,\n","    'metric': metric\n","}\n","save_parameters(HDBSCAN_params, params_file_path, 'HDBSCAN parameters')\n","\n","\n"],"metadata":{"cellView":"form","id":"MNdNepTiUBDF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eBFfmTo5e-Wv"},"source":["## **3.5. Fingerprint**\n","---\n","\n","<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"B8S0uwynjht1"},"outputs":[],"source":["# @title ##Plot the 'fingerprint' of each cluster per condition\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Group by 'Condition' and 'Cluster' and calculate the size of each group\n","cluster_counts = umap_df.groupby(['Condition', 'Cluster_UMAP']).size().reset_index(name='counts')\n","\n","# Calculate the total number of points per condition\n","total_counts = umap_df.groupby('Condition').size().reset_index(name='total_counts')\n","\n","# Merge the DataFrames on 'Condition' to calculate percentages\n","percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n","percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n","\n","# Save the percentage_df DataFrame as a CSV file\n","percentage_df.to_csv(Results_Folder+'/UMAP/UMAP_percentage_results.csv', index=False)\n","\n","# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n","pivot_df = percentage_df.pivot(index='Condition', columns='Cluster_UMAP', values='percentage')\n","\n","# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n","pivot_df.fillna(0, inplace=True)\n","\n","# Initialize PDF\n","pdf_pages = PdfPages(Results_Folder+'/UMAP/UMAP_Cluster_Fingerprint_Plot.pdf')\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(10, 7))\n","pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n","plt.title('Percentage in each cluster per Condition')\n","plt.ylabel('Percentage')\n","plt.xlabel('Condition')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","\n","ax.legend(title='Cluster_UMAP', loc='upper left', bbox_to_anchor=(1, 1))\n","\n","\n","# Save the figure to a PDF\n","pdf_pages.savefig(fig, bbox_inches='tight')\n","\n","# Close the PDF\n","pdf_pages.close()\n","\n","# Display the plot\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"huCM6iMWSd_j"},"source":["## **3.6. Understand your clusters using heatmaps**\n","--------\n","\n","<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RYgcVxG-meLP"},"outputs":[],"source":["import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import pandas as pd\n","from scipy.stats import zscore\n","\n","# @title ##Plot normalized numeric data based on clusters as an heatmap\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/UMAP\"\n","Conditions = 'Cluster_UMAP'\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def heatmap_comparison(df, Results_Folder, Conditions, variables_per_page=40):\n","    # Get all the selectable columns\n","    variables_to_plot = get_selectable_columns(df)\n","\n","    # Drop rows where all elements are NaNs in the variables_to_plot columns\n","    df = df.dropna()\n","\n","    # Compute median for each variable across Clusters\n","    median_values = df.groupby(Conditions)[variables_to_plot].median().transpose()\n","\n","    # Normalize the median values using Z-score\n","    normalized_values = median_values.apply(zscore, axis=1)\n","\n","    # Number of pages\n","    total_variables = len(variables_to_plot)\n","    num_pages = int(np.ceil(total_variables / variables_per_page))\n","\n","    # Initialize an empty DataFrame to store all pages' data\n","    all_pages_data = pd.DataFrame()\n","\n","    # Create a PDF file to save the heatmaps\n","    with PdfPages(f\"{Results_Folder}/Heatmaps_Normalized_Median_Values_by_Cluster.pdf\") as pdf:\n","        for page in range(num_pages):\n","            start = page * variables_per_page\n","            end = min(start + variables_per_page, total_variables)\n","            page_data = normalized_values.iloc[start:end]\n","\n","            # Append this page's data to the all_pages_data DataFrame\n","            all_pages_data = pd.concat([all_pages_data, page_data])\n","\n","            plt.figure(figsize=(16, 10))\n","            sns.heatmap(page_data, cmap='coolwarm', annot=True, linewidths=.1)\n","            plt.title(f\"Z-score Normalized Median Values of Variables by Condition (Page {page + 1})\")\n","            plt.tight_layout()\n","\n","            pdf.savefig()  # saves the current figure into a pdf page\n","            plt.show()\n","            plt.close()\n","\n","    # Save all pages data to a single CSV file\n","    all_pages_data.to_csv(f\"{Results_Folder}/Normalized_Median_Values_by_Condition.csv\")\n","\n","    print(f\"Heatmaps saved to {Results_Folder}/Heatmap_Normalized_Median_Values_by_Cluster.pdf\")\n","    print(f\"All data saved to {Results_Folder}/Normalized_Median_Values_by_Cluster_All.csv\")\n","\n","# Example usage\n","heatmap_comparison(dataset_df, base_folder, Conditions)"]},{"cell_type":"markdown","metadata":{"id":"5KWMzPMaR3OC"},"source":["## **3.7. Understand your clusters using box plots**\n","--------\n","\n","<font size = 4>The provided code aims to visually represent the distribution of different numeric data across the identified clusters. Specifically, for each numeric data selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each numeric data varies within and across the clusters.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"e2jMmapvz8Sh"},"outputs":[],"source":["import os\n","import itertools\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.gridspec import GridSpec\n","import pandas as pd\n","import ipywidgets as widgets\n","\n","# @title ##Plot numeric data based on clusters\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/UMAP/Data_by_clusters\"):\n","    os.makedirs(f\"{Results_Folder}/UMAP/Data_by_clusters\")\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Results_Folder):\n","    print(\"Plotting in progress...\")\n","\n","    # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","    for var in variables_to_plot:\n","        # Extract data for the specific variable and cluster\n","        data_to_save = df[['Cluster_UMAP', var]]\n","\n","        # Save data for the plot to CSV\n","        data_to_save.to_csv(f\"{Results_Folder}/UMAP/Data_by_clusters/{var}_data_by_Cluster.csv\", index=False)\n","\n","        plt.figure(figsize=(16, 10))\n","\n","        # Plotting\n","        sns.boxplot(x='Cluster_UMAP', y=var, data=df, color='lightgray')  # Boxplot by cluster\n","        sns.stripplot(x='Cluster_UMAP', y=var, data=df, jitter=True, alpha=0.2)  # Individual data points\n","\n","        plt.title(f\"{var} by Cluster\")\n","        plt.xlabel('Cluster_UMAP')\n","        plt.ylabel(var)\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","\n","        # Save the plot\n","        plt.savefig(f\"{Results_Folder}/UMAP/Data_by_clusters/{var}_Boxplots_by_Cluster.pdf\")\n","        plt.show()\n","\n","selectable_columns = get_selectable_columns(dataset_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","# Create and display the plot button\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, dataset_df, Results_Folder))\n","display(button)\n"]},{"cell_type":"markdown","metadata":{"id":"dKU37w3jnuh_"},"source":["## **3.8. Plot numeric data for a selected cluster**\n","---"]},{"cell_type":"code","source":["# @title ##Plot numeric data for a selected cluster\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.ticker import FixedLocator\n","\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/UMAP/Data_selected_cluster\"\n","Conditions = 'Condition'\n","df_to_plot = dataset_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def display_cluster_dropdown(df):\n","    # Extract unique clusters\n","    unique_clusters = df['Cluster_UMAP'].unique()\n","    cluster_dropdown = widgets.Dropdown(\n","        options=unique_clusters,\n","        description='Select Cluster:',\n","        disabled=False,\n","    )\n","    display(cluster_dropdown)\n","    return cluster_dropdown\n","\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='80%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, cluster_dropdown, Results_Folder, condition_selector):\n","\n","    selected_cluster = cluster_dropdown.value\n","    print(f\"Plotting in progress for Cluster {selected_cluster}...\")\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[(df[Conditions].isin(selected_conditions)) & (df['Cluster_UMAP'] == selected_cluster)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/Cluster_{selected_cluster}_{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/Cluster_{selected_cluster}_{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat']]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/Cluster_{selected_cluster}_{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var} for Cluster {selected_cluster}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","cluster_dropdown = display_cluster_dropdown(dataset_df)\n","\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, cluster_dropdown, base_folder, condition_selector))\n","display(button)"],"metadata":{"cellView":"form","id":"f-ngXb1bMFw5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42rnP6i0EBh5"},"source":["--------\n","# **Part 4. Explore your high-dimensional data using t-SNE and HDBSCAN**\n","--------"]},{"cell_type":"markdown","metadata":{"id":"pbVpn2JbArpc"},"source":["## **4.1. Choose the conditions to use**\n","\n"]},{"cell_type":"code","source":["#@markdown ##Choose the conditions to use:\n","\n","import pandas as pd\n","import ipywidgets as widgets\n","from IPython.display import display\n","import os\n","\n","TSNE_folder_path = os.path.join(Results_Folder, \"TSNE\")\n","if not os.path.exists(TSNE_folder_path):\n","    os.makedirs(TSNE_folder_path)\n","\n","# Function to parse the text area content into a list\n","def parse_text_area(text):\n","    return [item.strip() for item in text.split(',') if item.strip()]\n","\n","# Text area for user to paste the list of conditions\n","text_area_conditions = widgets.Textarea(\n","    value='',\n","    placeholder='Copy and paste your list of conditions here, separated by commas. Or tick the boxes below.',\n","    description='Conditions:',\n","    disabled=False,\n","    layout=widgets.Layout(width='80%', height='100px')\n",")\n","\n","# Create checkboxes for each unique condition in the dataset\n","Condition_checkboxes = [widgets.Checkbox(value=True, description=str(condition)) for condition in dataset_df['Condition'].unique()]\n","\n","# Function to filter dataframe based on selected checkbox values and text area input\n","def filter_dataframe(button):\n","    global filtered_df\n","    # Initialize an empty list to hold selected conditions\n","    selected_conditions = []\n","\n","    # Check if the text area is not empty\n","    if text_area_conditions.value.strip():\n","        # Use conditions from the text area\n","        selected_conditions = parse_text_area(text_area_conditions.value)\n","    else:\n","        # Use conditions from checkboxes if the text area is empty\n","        selected_conditions = [box.description for box in Condition_checkboxes if box.value]\n","\n","    # Filter DataFrame\n","    filtered_df = dataset_df[dataset_df['Condition'].isin(selected_conditions)]\n","\n","    print(\"Selected Conditions:\", selected_conditions)\n","    print(\"Filtered DataFrame length:\", len(filtered_df))\n","    if len(filtered_df) == 0:\n","        print(\"No data matched the selected filters. Check filters and data for consistency.\")\n","\n","    # Save selected conditions as parameters\n","    params = {'Selected Conditions': ', '.join(selected_conditions)}\n","    params_file_path = os.path.join(TSNE_folder_path, \"TSNE_analysis_parameters.csv\")\n","    save_parameters(params, params_file_path, 'TSNE Conditions')\n","\n","# Button to trigger dataframe filtering\n","filter_button = widgets.Button(description=\"Filter Dataframe\")\n","filter_button.on_click(filter_dataframe)\n","\n","# Display text area, checkboxes, and button\n","display(text_area_conditions, widgets.VBox([widgets.Label('Select Conditions:')] + Condition_checkboxes + [filter_button]))\n"],"metadata":{"cellView":"form","id":"axqZf41QArpq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4.2. Choose the numeric data to use**\n","\n"],"metadata":{"id":"JqPkIdeEArpq"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","#@markdown ##Choose the numeric data to use\n","\n","\n","TSNE_folder_path = os.path.join(Results_Folder, \"TSNE\")\n","if not os.path.exists(TSNE_folder_path):\n","    os.makedirs(TSNE_folder_path)\n","\n","# Define columns to preserve and to exclude if present\n","preserve_columns = ['Condition']\n","exclude_columns = ['Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","\n","# Ensure 'Condition' column and potentially 'Repeat' column are handled in 'numeric_df' for later use\n","numeric_columns = filtered_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n","# Keep a copy of the 'Condition' column along with numeric columns\n","# Exclude 'Repeat' column if it exists\n","numeric_df = filtered_df[numeric_columns + preserve_columns].copy()\n","if 'Repeat' in numeric_df.columns:\n","    numeric_df = numeric_df.drop(columns=['Repeat'])\n","\n","# Text area for user to paste the list of metrics\n","text_area = widgets.Textarea(\n","    value='',\n","    placeholder='Copy and paste your list of metrics here, separated by commas. Or tick the boxes below',\n","    description='Metrics:',\n","    disabled=False,\n","    layout=widgets.Layout(width='80%', height='100px')\n",")\n","\n","# Function to parse the text area content into a list\n","def parse_text_area(text):\n","    return [item.strip() for item in text.split(',') if item.strip() in numeric_columns]\n","\n","# Create a checkbox for each numeric column (excluding 'Condition' and potentially 'Repeat')\n","checkboxes = [widgets.Checkbox(value=True, description=col, indent=False) for col in numeric_columns if col not in exclude_columns]\n","\n","# Arrange checkboxes in a grid\n","grid = widgets.GridBox(checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(2, 300px)\"))\n","\n","# Create a button to trigger the selection\n","button = widgets.Button(description=\"Select Track Parameters\", layout=widgets.Layout(width='400px'))\n","\n","# Define the button click event handler\n","def on_button_click(b):\n","    global selected_df\n","\n","    # Use metrics from the text area if not empty, else from checkboxes\n","    if text_area.value.strip():\n","        selected_columns = parse_text_area(text_area.value)\n","    else:\n","        selected_columns = [box.description for box in checkboxes if box.value]\n","\n","    selected_df = numeric_df[selected_columns + preserve_columns].copy()\n","\n","    # Check for NaN values and handle them\n","    nan_columns = selected_df.columns[selected_df.isna().any()].tolist()\n","    if nan_columns:\n","        for col in nan_columns:\n","            selected_df.dropna(subset=[col], inplace=True)  # Drop rows with NaN values in these columns\n","            print(f\"Dropped rows from column '{col}' due to NaN values.\")\n","\n","    print(\"Track parameters selected and NaN values handled.\")\n","    params = {'Selected numeric data': ', '.join(selected_columns)}\n","    params_file_path = os.path.join(TSNE_folder_path, \"TSNE_analysis_parameters.csv\")\n","    save_parameters(params, params_file_path, 'TSNE numeric data')\n","\n","# Set the button click event handler\n","button.on_click(on_button_click)\n","\n","# Display the text area, grid of checkboxes, and the button\n","display(text_area, grid, button)\n"],"metadata":{"cellView":"form","id":"tsmTnW4AArpq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBPMuhoYIV7A"},"source":["## **4.3. t-SNE**\n","--------\n","\n","The code snippet provided performs **t-Distributed Stochastic Neighbor Embedding (t-SNE)**, a powerful technique for dimensionality reduction, particularly suited for the visualization of high-dimensional datasets. The process is applied to the merged tracks dataframe, focusing on its numeric columns, with the goal of visualizing the data in a lower-dimensional space.\n","\n","### Key Parameters of t-SNE:\n","\n","- **Perplexity (`perplexity`):**\n","  - This parameter is a measure of the effective number of local neighbors each point has.\n","  - Perplexity influences the t-SNE algorithm's ability to capture local versus global aspects of the data.\n","  - Typical values for perplexity range between 5 and 50, with the choice depending on dataset size and density.\n","\n","- **Learning Rate (`learning_rate`):**\n","  - This parameter controls the step size in the optimization process.\n","  - A suitable learning rate helps t-SNE to converge to a meaningful low-dimensional representation.\n","  - Values too high might cause the algorithm to converge to a suboptimal solution, while too low values can slow down the convergence.\n","\n","- **Number of Iterations (`n_iter`):**\n","  - This parameter defines the number of optimization iterations t-SNE will run.\n","  - A higher number of iterations allows the algorithm more time to find a stable configuration.\n","  - Generally, a value of 1000 iterations is sufficient for most datasets.\n","\n","- **Number of Dimensions (`n_dimension`):**\n","  - The target dimensionality for the lower-dimensional space.\n","  - For visualization purposes, this is commonly set to 2, allowing the data to be plotted in a 2D scatter plot.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"p9vuULJXE1XG"},"outputs":[],"source":["# @title ##Perform t-SNE\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import warnings\n","import pandas as pd\n","\n","# Check and create necessary directories\n","tsne_folder_path = f\"{Results_Folder}/TSNE/\"\n","if not os.path.exists(tsne_folder_path):\n","    os.makedirs(tsne_folder_path)\n","\n","#@markdown ###t-SNE parameters:\n","\n","perplexity = 20  # @param {type: \"number\"}\n","learning_rate = 100  # @param {type: \"number\"}\n","n_iter = 1000  # @param {type: \"number\"}\n","n_dimension = 2  # The number of dimensions is set to 2 for t-SNE as standard practice\n","\n","#@markdown ###Display parameters:\n","spot_size = 15  # @param {type: \"number\"}\n","\n","# Initialize t-SNE object with the specified settings\n","tsne = TSNE(n_components=n_dimension, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=42)\n","\n","# Exclude non-numeric columns when fitting t-SNE\n","numeric_columns = selected_df._get_numeric_data()\n","embedding = tsne.fit_transform(numeric_columns)\n","\n","  # Prepare the parameters dictionary\n","tsne_params = {\n","        'perplexity': perplexity,\n","        'learning_rate': learning_rate,\n","        'n_iter': n_iter,\n","        'n_dimension': n_dimension,\n","        'spot_size': spot_size\n","    }\n","\n","    # Save the parameters\n","params_file_path = os.path.join(tsne_folder_path, \"TSNE_analysis_parameters.csv\")\n","save_parameters(tsne_params, params_file_path, 'TSNE')\n","\n","# Create dynamic column names based on n_components\n","column_names = [f't-SNE dimension {i+1}' for i in range(n_dimension)]\n","\n","# Extract the columns_to_include from selected_df\n","included_data = selected_df.reset_index(drop=True)\n","\n","# Concatenate the t-SNE embedding with the included columns\n","tsne_df = pd.concat([pd.DataFrame(embedding, columns=column_names), included_data], axis=1)\n","\n","# Check if the DataFrame has any NaN values and print a warning if it does.\n","nan_columns = tsne_df.columns[tsne_df.isna().any()].tolist()\n","if nan_columns:\n","  warnings.warn(f\"The DataFrame contains NaN values in the following columns: {', '.join(nan_columns)}\")\n","  tsne_df.dropna(subset=nan_columns, inplace=True)  # Drop NaN values only from columns containing them\n","\n","# Visualize the t-SNE projection\n","plt.figure(figsize=(12, 10))\n","sns.scatterplot(x=column_names[0], y=column_names[1], hue='Condition', data=tsne_df, palette='Set2', s=spot_size)\n","plt.title('t-SNE Projection of the Dataset')\n","tsne_output_path = os.path.join(tsne_folder_path, 'TSNE_projection_2D.pdf')\n","plt.savefig(tsne_output_path)  # Save 2D plot as PDF\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"-Cs7TLJAKiGR"},"source":["## **4.4. HDBSCAN**\n","---\n","\n","<font size = 4> The provided code employs HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to identify clusters within a dataset that has already undergone UMAP dimensionality reduction. HDBSCAN is utilized for its proficiency in determining the optimal number of clusters while managing varied densities within the data.\n","\n","<font size = 4>In the provided HDBSCAN code, the parameters `min_samples`, `min_cluster_size`, and `metric` are crucial for determining the structure and appearance of the resulting clusters in the data.\n","\n","<font size = 4>`min_samples`: This parameter primarily controls the degree to which the algorithm is willing to declare noise. It's the number of samples in a neighborhood for a point to be considered as a core point.\n","- A smaller value of `min_samples` makes the algorithm more prone to declaring points as part of a cluster, potentially leading to larger clusters and fewer noise points.\n","- A larger value makes the algorithm more conservative, resulting in more points declared as noise and smaller, more defined clusters.\n","- The choice of `min_samples` typically depends on the density of the data; denser datasets may require a larger value.\n","\n","<font size = 4>`min_cluster_size`: This parameter determines the smallest size grouping that you wish to consider a cluster.\n","- A smaller value will allow the formation of smaller clusters, whereas a larger value will prevent small isolated groups of points from being declared as clusters.\n","- The choice of `min_cluster_size` depends on the scale of the data and the desired level of granularity in the clustering.\n","\n","<font size = 4>`metric`: This parameter is the metric used for distance computation between data points, and it affects the shape of the clusters.\n","- The `euclidean` metric is a good starting point, and depending on the clustering results and the data type, it might be beneficial to experiment with different metrics.\n"]},{"cell_type":"code","source":["import hdbscan\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","# @title ##Identify clusters using HDBSCAN\n","\n","#@markdown ###HDBSCAN parameters:\n","clustering_data_source = 'tsne'  # @param ['tsne', 'raw']\n","min_samples = 10  # @param {type: \"number\"}\n","min_cluster_size = 100  # @param {type: \"number\"}\n","metric = \"euclidean\"  # @param ['euclidean', 'manhattan', 'chebyshev', 'braycurtis', 'canberra']\n","\n","#@markdown ###Display parameters:\n","spot_size = 20 # @param {type: \"number\"}\n","\n","tsne_folder_path = os.path.join(Results_Folder, \"TSNE\")\n","os.makedirs(tsne_folder_path, exist_ok=True)\n","\n","column_names = [f't-SNE dimension {i+1}' for i in range(n_dimension)]\n","\n","# Initialize HDBSCAN\n","clusterer = hdbscan.HDBSCAN(min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric)\n","\n","# Use the correct column names for clustering\n","data_for_clustering = tsne_df[column_names]\n","\n","# Apply clustering\n","clusterer.fit(data_for_clustering)\n","\n","# Add the cluster labels to tsne_df\n","tsne_df['Cluster_TSNE'] = clusterer.labels_\n","\n","# Plotting the results\n","if n_dimension == 1:\n","    plt.figure(figsize=(12, 6))\n","    sns.stripplot(x=column_names[0], hue='Cluster_TSNE', data=tsne_df, palette='viridis', size=spot_size)\n","    plt.title('Clusters Identified by HDBSCAN (1D)')\n","    plt.savefig(os.path.join(tsne_folder_path, \"HDBSCAN_clusters_1D.pdf\"))\n","elif n_dimension == 2:\n","    plt.figure(figsize=(12, 10))\n","    sns.scatterplot(x=column_names[0], y=column_names[1], hue='Cluster_TSNE', data=tsne_df, palette='viridis', size=spot_size)\n","    plt.title('Clusters Identified by HDBSCAN (2D)')\n","    plt.savefig(os.path.join(tsne_folder_path, \"HDBSCAN_clusters_2D.pdf\"))\n","elif n_dimension == 3:\n","    fig = px.scatter_3d(tsne_df, x=column_names[0], y=column_names[1], z=column_names[2], color='Cluster_TSNE', size_max=spot_size)\n","    fig.update_traces(marker=dict(size=spot_size/10))\n","    fig.show()\n","    fig.write_html(os.path.join(tsne_folder_path, \"HDBSCAN_clusters_3D.html\"), auto_open=False)\n","else:\n","    print(\"Invalid n_dimension value for clustering visualization.\")\n","\n","plt.show()\n","\n","dataset_df.reset_index(drop=True, inplace=True)\n","tsne_df.reset_index(drop=True, inplace=True)\n","\n","# If the Cluster column already exists in dataset_df, drop it to avoid duplications\n","if 'Cluster_TSNE' in dataset_df.columns:\n","    dataset_df.drop(columns='Cluster_TSNE', inplace=True)\n","\n","# Directly join the 'Cluster_UMAP' column to dataset_df based on index\n","dataset_df['Cluster_TSNE'] = tsne_df['Cluster_TSNE']\n","\n","# Optionally, fill NaN values with -1 if there are unassigned clusters\n","dataset_df['Cluster_TSNE'].fillna(-1, inplace=True)\n","\n","# Save the DataFrame with the identified clusters\n","dataset_df.to_csv(f\"{Results_Folder}/merged_dataset_clusters.csv\", index=False)\n","\n","# Save clustering parameters\n","params_file_path = os.path.join(tsne_folder_path, \"TSNE_analysis_parameters.csv\")\n","\n","# Save clustering parameters and results\n","HDBSCAN_params = {\n","    'clustering_data_source': clustering_data_source,\n","    'min_samples': min_samples,\n","    'min_cluster_size': min_cluster_size,\n","    'metric': metric\n","}\n","save_parameters(HDBSCAN_params, params_file_path, 'HDBSCAN parameters')\n"],"metadata":{"cellView":"form","id":"qn8zqtvgCcTt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWLXOFtoCcT8"},"source":["## **3.5. Fingerprint**\n","---\n","\n","<font size = 4>This section is designed to visualize the distribution of different clusters within each condition in a dataset, showing the 'fingerprint' of each cluster per condition."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"H-SzS7Q1CcT8"},"outputs":[],"source":["# @title ##Plot the 'fingerprint' of each cluster per condition\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","# Group by 'Condition' and 'Cluster' and calculate the size of each group\n","cluster_counts = tsne_df.groupby(['Condition', 'Cluster_TSNE']).size().reset_index(name='counts')\n","\n","# Calculate the total number of points per condition\n","total_counts = tsne_df.groupby('Condition').size().reset_index(name='total_counts')\n","\n","# Merge the DataFrames on 'Condition' to calculate percentages\n","percentage_df = pd.merge(cluster_counts, total_counts, on='Condition')\n","percentage_df['percentage'] = (percentage_df['counts'] / percentage_df['total_counts']) * 100\n","\n","# Save the percentage_df DataFrame as a CSV file\n","percentage_df.to_csv(Results_Folder+'/TSNE/TSNE_percentage_results.csv', index=False)\n","\n","# Pivot the percentage_df to have Conditions as index, Clusters as columns, and percentages as values\n","pivot_df = percentage_df.pivot(index='Condition', columns='Cluster_TSNE', values='percentage')\n","\n","# Fill NaN values with 0 if any, as there might be some Condition-Cluster combinations that are not present\n","pivot_df.fillna(0, inplace=True)\n","\n","# Initialize PDF\n","pdf_pages = PdfPages(Results_Folder+'/TSNE/TSNE_Cluster_Fingerprint_Plot.pdf')\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(10, 7))\n","pivot_df.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n","plt.title('Percentage in each cluster per Condition')\n","plt.ylabel('Percentage')\n","plt.xlabel('Condition')\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","\n","ax.legend(title='Cluster_TSNE', loc='upper left', bbox_to_anchor=(1, 1))\n","\n","\n","# Save the figure to a PDF\n","pdf_pages.savefig(fig, bbox_inches='tight')\n","\n","# Close the PDF\n","pdf_pages.close()\n","\n","# Display the plot\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qnG7IvfBCcT8"},"source":["## **3.6. Understand your clusters using heatmaps**\n","--------\n","\n","<font size = 4>This section help visualize how different track parameters vary across the identified clusters. The approach is to display these variations using a heatmap, which offers a color-coded representation of the median values of each parameter for each cluster. This visualization technique can make it easier to spot differences or patterns among the clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"DzRQMiC9CcT8"},"outputs":[],"source":["import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","import pandas as pd\n","from scipy.stats import zscore\n","\n","# @title ##Plot normalized numeric data based on clusters as an heatmap\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/TSNE\"\n","Conditions = 'Cluster_TSNE'\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_TSNE']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def heatmap_comparison(df, Results_Folder, Conditions, variables_per_page=40):\n","    # Get all the selectable columns\n","    variables_to_plot = get_selectable_columns(df)\n","\n","    # Drop rows where all elements are NaNs in the variables_to_plot columns\n","    df = df.dropna()\n","\n","    # Compute median for each variable across Clusters\n","    median_values = df.groupby(Conditions)[variables_to_plot].median().transpose()\n","\n","    # Normalize the median values using Z-score\n","    normalized_values = median_values.apply(zscore, axis=1)\n","\n","    # Number of pages\n","    total_variables = len(variables_to_plot)\n","    num_pages = int(np.ceil(total_variables / variables_per_page))\n","\n","    # Initialize an empty DataFrame to store all pages' data\n","    all_pages_data = pd.DataFrame()\n","\n","    # Create a PDF file to save the heatmaps\n","    with PdfPages(f\"{Results_Folder}/Heatmaps_Normalized_Median_Values_by_Cluster.pdf\") as pdf:\n","        for page in range(num_pages):\n","            start = page * variables_per_page\n","            end = min(start + variables_per_page, total_variables)\n","            page_data = normalized_values.iloc[start:end]\n","\n","            # Append this page's data to the all_pages_data DataFrame\n","            all_pages_data = pd.concat([all_pages_data, page_data])\n","\n","            plt.figure(figsize=(16, 10))\n","            sns.heatmap(page_data, cmap='coolwarm', annot=True, linewidths=.1)\n","            plt.title(f\"Z-score Normalized Median Values of Variables by Condition (Page {page + 1})\")\n","            plt.tight_layout()\n","\n","            pdf.savefig()  # saves the current figure into a pdf page\n","            plt.show()\n","            plt.close()\n","\n","    # Save all pages data to a single CSV file\n","    all_pages_data.to_csv(f\"{Results_Folder}/Normalized_Median_Values_by_Condition.csv\")\n","\n","    print(f\"Heatmaps saved to {Results_Folder}/Heatmap_Normalized_Median_Values_by_Cluster.pdf\")\n","    print(f\"All data saved to {Results_Folder}/Normalized_Median_Values_by_Cluster_All.csv\")\n","\n","# Example usage\n","heatmap_comparison(dataset_df, base_folder, Conditions)"]},{"cell_type":"markdown","metadata":{"id":"RZdpYv4YCcT9"},"source":["## **3.7. Understand your clusters using box plots**\n","--------\n","\n","<font size = 4>The provided code aims to visually represent the distribution of different numeric data across the identified clusters. Specifically, for each numeric data selected, a boxplot is generated to showcase the spread of its values across different clusters. This approach provides a comprehensive view of how each numeric data varies within and across the clusters.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MfHkOV7yCcT9"},"outputs":[],"source":["import os\n","import itertools\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.gridspec import GridSpec\n","import pandas as pd\n","import ipywidgets as widgets\n","\n","# @title ##Plot numeric data based on clusters\n","\n","# Check and create \"pdf\" folder\n","if not os.path.exists(f\"{Results_Folder}/TSNE/Data_by_clusters\"):\n","    os.makedirs(f\"{Results_Folder}/TSNE/Data_by_clusters\")\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Results_Folder):\n","    print(\"Plotting in progress...\")\n","\n","    # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","    for var in variables_to_plot:\n","        # Extract data for the specific variable and cluster\n","        data_to_save = df[['Cluster_TSNE', var]]\n","\n","        # Save data for the plot to CSV\n","        data_to_save.to_csv(f\"{Results_Folder}/TSNE/Data_by_clusters/{var}_data_by_Cluster.csv\", index=False)\n","\n","        plt.figure(figsize=(16, 10))\n","\n","        # Plotting\n","        sns.boxplot(x='Cluster_TSNE', y=var, data=df, color='lightgray')  # Boxplot by cluster\n","        sns.stripplot(x='Cluster_TSNE', y=var, data=df, jitter=True, alpha=0.2)  # Individual data points\n","\n","        plt.title(f\"{var} by Cluster\")\n","        plt.xlabel('Cluster_TSNE')\n","        plt.ylabel(var)\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","\n","        # Save the plot\n","        plt.savefig(f\"{Results_Folder}/TSNE/Data_by_clusters/{var}_Boxplots_by_Cluster.pdf\")\n","        plt.show()\n","\n","selectable_columns = get_selectable_columns(dataset_df)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","\n","# Create and display the plot button\n","button = widgets.Button(description=\"Plot Selected Variables\", layout=widgets.Layout(width='400px'))\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, dataset_df, Results_Folder))\n","display(button)\n"]},{"cell_type":"markdown","metadata":{"id":"B2Y8v_8iCcT9"},"source":["## **3.8. Plot numeric data for a selected cluster**\n","---"]},{"cell_type":"code","source":["# @title ##Plot numeric data for a selected cluster\n","\n","import ipywidgets as widgets\n","from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n","import pandas as pd\n","import os\n","from matplotlib.backends.backend_pdf import PdfPages\n","from matplotlib.ticker import FixedLocator\n","\n","\n","# Parameters to adapt in function of the notebook section\n","base_folder = f\"{Results_Folder}/TSNE/Data_selected_cluster\"\n","Conditions = 'Condition'\n","df_to_plot = dataset_df\n","\n","# Check and create necessary directories\n","folders = [\"pdf\", \"csv\"]\n","for folder in folders:\n","    dir_path = os.path.join(base_folder, folder)\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","\n","def get_selectable_columns(df):\n","    # Exclude certain columns from being plotted\n","    exclude_cols = ['Condition', 'Repeat', 'Cluster_UMAP', 'Cluster_TSNE']\n","    # Select only numerical columns\n","    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n","\n","\n","def display_cluster_dropdown(df):\n","    # Extract unique clusters\n","    unique_clusters = df['Cluster_TSNE'].unique()\n","    cluster_dropdown = widgets.Dropdown(\n","        options=unique_clusters,\n","        description='Select Cluster:',\n","        disabled=False,\n","    )\n","    display(cluster_dropdown)\n","    return cluster_dropdown\n","\n","\n","def display_variable_checkboxes(selectable_columns):\n","    # Create checkboxes for selectable columns\n","    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n","\n","    # Display checkboxes in the notebook\n","    display(widgets.VBox([\n","        widgets.Label('Variables to Plot:'),\n","        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n","    ]))\n","    return variable_checkboxes\n","\n","def create_condition_selector(df, column_name):\n","    conditions = df[column_name].unique()\n","    condition_selector = SelectMultiple(\n","        options=conditions,\n","        description='Conditions:',\n","        disabled=False,\n","        layout=Layout(width='80%')  # Adjusting the layout width\n","    )\n","    return condition_selector\n","\n","def display_condition_selection(df, column_name):\n","    condition_selector = create_condition_selector(df, column_name)\n","\n","    condition_accordion = Accordion(children=[VBox([condition_selector])])\n","    condition_accordion.set_title(0, 'Select Conditions')\n","    display(condition_accordion)\n","    return condition_selector\n","\n","\n","def plot_selected_vars(button, variable_checkboxes, df, Conditions, cluster_dropdown, Results_Folder, condition_selector):\n","\n","    selected_cluster = cluster_dropdown.value\n","    print(f\"Plotting in progress for Cluster {selected_cluster}...\")\n","\n","    plt.clf()  # Clear the current figure before creating a new plot\n","\n","\n","  # Get selected variables\n","    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n","    n_plots = len(variables_to_plot)\n","\n","    if n_plots == 0:\n","        print(\"No variables selected for plotting\")\n","        return\n","\n","  # Get selected conditions\n","    selected_conditions = condition_selector.value\n","    n_selected_conditions = len(selected_conditions)\n","\n","    if n_selected_conditions == 0:\n","        print(\"No conditions selected for plotting\")\n","        return\n","\n","# Use only selected and ordered conditions\n","    filtered_df = df[(df[Conditions].isin(selected_conditions)) & (df['Cluster_TSNE'] == selected_cluster)].copy()\n","\n","# Initialize matrices to store effect sizes and p-values for each variable\n","    effect_size_matrices = {}\n","    p_value_matrices = {}\n","    bonferroni_matrices = {}\n","\n","    unique_conditions = filtered_df[Conditions].unique().tolist()\n","    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n","    alpha = 0.05\n","    corrected_alpha = alpha / num_comparisons\n","    n_iterations = 1000\n","\n","# Loop through each variable to plot\n","    for var in variables_to_plot:\n","\n","      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/Cluster_{selected_cluster}_{var}_Boxplots_and_Statistics.pdf\")\n","      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n","\n","      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n","        group1 = df[df[Conditions] == cond1][var]\n","        group2 = df[df[Conditions] == cond2][var]\n","\n","        original_d = abs(cohen_d(group1, group2))\n","        effect_size_matrix.loc[cond1, cond2] = original_d\n","        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n","\n","        count_extreme = 0\n","        for i in range(n_iterations):\n","            combined = pd.concat([group1, group2])\n","            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n","            new_group1 = shuffled[:len(group1)]\n","            new_group2 = shuffled[len(group1):]\n","\n","            new_d = abs(cohen_d(new_group1, new_group2))\n","            if np.abs(new_d) >= np.abs(original_d):\n","                count_extreme += 1\n","\n","        p_value = count_extreme / n_iterations\n","        p_value_matrix.loc[cond1, cond2] = p_value\n","        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n","\n","        # Apply Bonferroni correction\n","        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n","        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n","        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n","\n","      effect_size_matrices[var] = effect_size_matrix\n","      p_value_matrices[var] = p_value_matrix\n","      bonferroni_matrices[var] = bonferroni_matrix\n","\n","    # Concatenate the three matrices side-by-side\n","      combined_df = pd.concat(\n","        [\n","            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n","            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n","            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n","        ], axis=1\n","    )\n","\n","    # Save the combined DataFrame to a CSV file\n","      combined_df.to_csv(f\"{Results_Folder}/csv/Cluster_{selected_cluster}_{var}_statistics_combined.csv\")\n","\n","    # Create a new figure\n","      fig = plt.figure(figsize=(16, 10))\n","\n","    # Create a gridspec for 2 rows and 4 columns\n","      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n","\n","    # Create the ax for boxplot using the gridspec\n","      ax_box = fig.add_subplot(gs[0, :])\n","\n","    # Extract the data for this variable\n","      data_for_var = df[[Conditions, var, 'Repeat']]\n","\n","    # Save the data_for_var to a CSV for replotting\n","      data_for_var.to_csv(f\"{Results_Folder}/csv/Cluster_{selected_cluster}_{var}_boxplot_data.csv\", index=False)\n","\n","    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n","      Q1 = df[var].quantile(0.25)\n","      Q3 = df[var].quantile(0.75)\n","      IQR = Q3 - Q1\n","\n","    # Define bounds for the outliers\n","      multiplier = 10\n","      lower_bound = Q1 - multiplier * IQR\n","      upper_bound = Q3 + multiplier * IQR\n","\n","    # Plotting\n","      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n","      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n","      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n","      ax_box.set_title(f\"{var} for Cluster {selected_cluster}\")\n","      ax_box.set_xlabel('Condition')\n","      ax_box.set_ylabel(var)\n","      tick_labels = ax_box.get_xticklabels()\n","      tick_locations = ax_box.get_xticks()\n","      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n","      ax_box.set_xticklabels(tick_labels, rotation=90)\n","      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n","\n","    # Statistical Analyses and Heatmaps\n","\n","    # Effect Size heatmap ax\n","      ax_d = fig.add_subplot(gs[1, 0])\n","      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n","      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n","\n","    # p-value heatmap ax\n","      ax_p = fig.add_subplot(gs[1, 1])\n","      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n","      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n","\n","    # Bonferroni corrected p-value heatmap ax\n","      ax_bonf = fig.add_subplot(gs[1, 2])\n","      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n","      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n","\n","      plt.tight_layout()\n","      pdf_pages.savefig(fig)\n","\n","    # Close the PDF\n","      pdf_pages.close()\n","\n","condition_selector = display_condition_selection(df_to_plot, Conditions)\n","selectable_columns = get_selectable_columns(df_to_plot)\n","variable_checkboxes = display_variable_checkboxes(selectable_columns)\n","cluster_dropdown = display_cluster_dropdown(dataset_df)\n","\n","\n","button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n","button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, cluster_dropdown, base_folder, condition_selector))\n","display(button)"],"metadata":{"cellView":"form","id":"ZI5nFWhaCcT9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cd817UHYhCGD"},"source":["# **Part 5. Version log**\n","---\n","<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n","  - This notebook may contain bugs.\n","  - Features are currently limited and will be expanded in future releases.\n","\n","<font size = 4>We encourage users to report any issues or suggestions for improvement. Please check the [repository](https://github.com/guijacquemet/CellTracksColab) regularly for updates and the latest version of this notebook.\n","\n","\n","<font size = 4>**Version 0.1**\n","This is the first release of this notebook.\n","\n","---"]}],"metadata":{"colab":{"provenance":[{"file_id":"1fFblyBiEW50U7prm2dTtiIiJFWI-Q8v-","timestamp":1709225779782}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}