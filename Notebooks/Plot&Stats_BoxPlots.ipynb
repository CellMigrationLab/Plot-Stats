{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF4zYMmXULP7"
      },
      "source": [
        "# **Plot&Stats - BoxPlots**\n",
        "---\n",
        "\n",
        "<font size = 4>Colab Notebook for Plotting data\n",
        "\n",
        "\n",
        "<font size = 4>Notebook created by [Guillaume Jacquemet](https://cellmig.org/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR2U8v9YoJcW"
      },
      "source": [
        "# **Part 0. Before getting started**\n",
        "---\n",
        "\n",
        "<font size = 5>**Important notes**\n",
        "\n",
        "---\n",
        "## Data Requirements for Analysis\n",
        "\n",
        "<font size = 4>For a successful analysis using this notebook, ensure your data meets the following criteria:\n",
        "\n",
        "## Notebook Data Format and Requirements Documentation\n",
        "\n",
        "This document details the prerequisites for data to be analyzed effectively within this notebook. Ensuring adherence to these guidelines will facilitate accurate and efficient data analysis.\n",
        "\n",
        "### File Format\n",
        "- **CSV**: Data should be in CSV (Comma-Separated Values) format, easily generated from spreadsheet applications (e.g., Excel, Google Sheets) or statistical software (e.g., R, Python).\n",
        "- **Copy and Paste**: Data can be directly copied and pasted from a spreedsheet software.\n",
        "\n",
        "### Data Structure: Tidy Format\n",
        "Data must follow the tidy data principles for optimal processing:\n",
        "- **Each Variable Forms a Column**: Every column represents a single variable.\n",
        "- **Each Observation Forms a Row**: Every row represents a single observation.\n",
        "- **Each Type of Observational Unit Forms a Table**: Different observational units should be in separate tables or clearly distinguishable.\n",
        "\n",
        "### Essential Columns\n",
        "Your dataset must include specific columns for analysis:\n",
        "- **Biological Repeat Column**: Identifies biological replicates. Names can vary (e.g., \"Repeat\", \"Bio_Replicate\") but must consistently identify each biological repeat.\n",
        "- **Condition Column**: Categorizes observations by experimental conditions or treatments. Names can vary (e.g., \"Condition\", \"Treatment\") but must provide clear, consistent categorization.\n",
        "\n",
        "### Data Preparation Tips\n",
        "- **Consistency and Clarity**: Ensure consistent and descriptive naming within \"Biological Repeat\" and \"Condition\" columns.\n",
        "- **Data Cleaning**: Address missing or erroneous entries in these essential columns to prevent analysis issues.\n",
        "\n",
        "### Column Naming Flexibility\n",
        "- The exact names of the \"Biological Repeat\" and \"Condition\" columns are flexible to fit various dataset structures and terminologies. You'll specify these columns when using the notebook.\n",
        "\n",
        "Adhering to these guidelines ensures your data is primed for the notebook's analytical capabilities, allowing for insightful comparisons across biological repeats and conditions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JrkfFr7mgZmA"
      },
      "outputs": [],
      "source": [
        "# @title #MIT License\n",
        "\n",
        "print(\"\"\"\n",
        "**MIT License**\n",
        "\n",
        "Copyright (c) 2023 Guillaume Jacquemet\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-Ft-yNRVCc"
      },
      "source": [
        "--------------------------------------------------------\n",
        "# **Part 1. Prepare the session and load your data**\n",
        "--------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h0prdayn0qG"
      },
      "source": [
        "## **1.1. Install key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_BZuYOQGo1p"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install\n",
        "%pip -q install pandas scikit-learn\n",
        "%pip -q install plotly\n",
        "%pip -q install prettytable\n",
        "#!pip -q install pmoss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAP0ahCzn1V6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to load the dependancies\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import itertools\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import requests\n",
        "from io import StringIO\n",
        "from IPython.display import display, clear_output\n",
        "import pandas as pd\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from prettytable import PrettyTable\n",
        "import os\n",
        "from tqdm.notebook import tqdm  # For the progress bar in save_dataframe_with_progress\n",
        "\n",
        "\n",
        "# Current version of the notebook the user is running\n",
        "current_version = \"0.3\"\n",
        "Notebook_name = 'BoxPlots'\n",
        "\n",
        "# URL to the raw content of the version file in the repository\n",
        "version_url = \"https://raw.githubusercontent.com/CellMigrationLab/Plot-Stats/main/Notebooks/latest_version.txt\"\n",
        "\n",
        "# Function to define colors for formatting messages\n",
        "class bcolors:\n",
        "    WARNING = '\\033[91m'  # Red color for warning messages\n",
        "    ENDC = '\\033[0m'      # Reset color to default\n",
        "\n",
        "# Check if this is the latest version of the notebook\n",
        "try:\n",
        "    All_notebook_versions = pd.read_csv(version_url, dtype=str)\n",
        "    print('Notebook version: ' + current_version)\n",
        "\n",
        "    # Check if 'Version' column exists in the DataFrame\n",
        "    if 'Version' in All_notebook_versions.columns:\n",
        "        Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Notebook_name]['Version'].iloc[0]\n",
        "        print('Latest notebook version: ' + Latest_Notebook_version)\n",
        "\n",
        "        if current_version == Latest_Notebook_version:\n",
        "            print(\"This notebook is up-to-date.\")\n",
        "        else:\n",
        "            print(bcolors.WARNING + \"A new version of this notebook has been released. We recommend that you download it at https://github.com/CellMigrationLab/Plot-Stats\" + bcolors.ENDC)\n",
        "    else:\n",
        "        print(\"The 'Version' column is not present in the version file.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"Unable to fetch the latest version information. Please check your internet connection.\")\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", str(e))\n",
        "\n",
        "\n",
        "\n",
        "# Function to calculate Cohen's d\n",
        "def cohen_d(group1, group2):\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var()\n",
        "    var2 = group2.var()\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "def save_dataframe_with_progress(df, path, desc=\"Saving\", chunk_size=50000):\n",
        "    \"\"\"Save a DataFrame with a progress bar.\"\"\"\n",
        "\n",
        "    # Estimating the number of chunks based on the provided chunk size\n",
        "    num_chunks = int(len(df) / chunk_size) + 1\n",
        "\n",
        "    # Create a tqdm instance for progress tracking\n",
        "    with tqdm(total=len(df), unit=\"rows\", desc=desc) as pbar:\n",
        "        # Open the file for writing\n",
        "        with open(path, \"w\") as f:\n",
        "            # Write the header once at the beginning\n",
        "            df.head(0).to_csv(f, index=False)\n",
        "\n",
        "            for chunk in np.array_split(df, num_chunks):\n",
        "                chunk.to_csv(f, mode=\"a\", header=False, index=False)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "def check_for_nans(df, df_name):\n",
        "    \"\"\"\n",
        "    Checks the given DataFrame for NaN values and prints the count for each column containing NaNs.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): DataFrame to be checked for NaN values.\n",
        "    df_name (str): The name of the DataFrame as a string, used for printing.\n",
        "    \"\"\"\n",
        "    # Check if the DataFrame has any NaN values and print a warning if it does.\n",
        "    nan_columns = df.columns[df.isna().any()].tolist()\n",
        "\n",
        "    if nan_columns:\n",
        "        for col in nan_columns:\n",
        "            nan_count = df[col].isna().sum()\n",
        "            print(f\"Column '{col}' in {df_name} contains {nan_count} NaN values.\")\n",
        "    else:\n",
        "        print(f\"No NaN values found in {df_name}.\")\n",
        "\n",
        "\n",
        "def save_parameters(params, file_path, param_type):\n",
        "    # Convert params dictionary to a DataFrame for human readability\n",
        "    new_params_df = pd.DataFrame(list(params.items()), columns=['Parameter', 'Value'])\n",
        "    new_params_df['Type'] = param_type\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        # Read existing file\n",
        "        existing_params_df = pd.read_csv(file_path)\n",
        "\n",
        "        # Merge the new parameters with the existing ones\n",
        "        # Update existing parameters or append new ones\n",
        "        updated_params_df = pd.merge(existing_params_df, new_params_df,\n",
        "                                     on=['Type', 'Parameter'],\n",
        "                                     how='outer',\n",
        "                                     suffixes=('', '_new'))\n",
        "\n",
        "        # If there's a new value, update it, otherwise keep the old value\n",
        "        updated_params_df['Value'] = updated_params_df['Value_new'].combine_first(updated_params_df['Value'])\n",
        "\n",
        "        # Drop the temporary new value column\n",
        "        updated_params_df.drop(columns='Value_new', inplace=True)\n",
        "    else:\n",
        "        # Use new parameters DataFrame directly if file doesn't exist\n",
        "        updated_params_df = new_params_df\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    updated_params_df.to_csv(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzd_8GUnpbw"
      },
      "source": [
        "## **1.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the instructions.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA1wCrkoV4i5"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /gdrive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDAwkSOo1gV"
      },
      "source": [
        "## **1.3. Load your dataset**\n",
        "---\n",
        "\n",
        "<font size = 4> Please ensure that your data is properly organised (see above)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yIX1uUc3NpCS"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Load your dataset:\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from io import StringIO\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Initialize dataset_df as an empty DataFrame globally\n",
        "dataset_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "# Create widgets\n",
        "dataset_path_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to your dataset',\n",
        "    description='Dataset Path:',\n",
        "    layout={'width': '80%'}\n",
        ")\n",
        "\n",
        "results_folder_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to your results folder',\n",
        "    description='Results Folder:',\n",
        "    layout={'width': '80%'}\n",
        ")\n",
        "\n",
        "data_textarea = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Or copy and paste your tab sperated data here (direct copy and paste from a spreedsheet)',\n",
        "    description='Or Paste Data:',\n",
        "    layout={'width': '80%', 'height': '200px'}\n",
        ")\n",
        "\n",
        "load_button = widgets.Button(\n",
        "    description='Load Data',\n",
        "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to load the data',\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "# Load data function\n",
        "def load_data(b):\n",
        "    global dataset_df\n",
        "    global Results_Folder\n",
        "\n",
        "    with output:\n",
        "        clear_output()\n",
        "        Results_Folder = results_folder_input.value.strip()\n",
        "        if not Results_Folder:\n",
        "            Results_Folder = './Results'  # Default path if not provided\n",
        "        if not os.path.exists(Results_Folder):\n",
        "            os.makedirs(Results_Folder)  # Create the folder if it doesn't exist\n",
        "        print(f\"Results folder is located at: {Results_Folder}\")\n",
        "\n",
        "        if dataset_path_input.value.strip():\n",
        "            dataset_path = dataset_path_input.value.strip()\n",
        "            try:\n",
        "                dataset_df = pd.read_csv(dataset_path)\n",
        "                print(f\"Loaded dataset from {dataset_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load dataset from {dataset_path}: {e}\")\n",
        "        elif data_textarea.value.strip():\n",
        "            input_data = StringIO(data_textarea.value)\n",
        "            try:\n",
        "                dataset_df = pd.read_csv(input_data, sep='\\t')\n",
        "                print(\"Loaded dataset from pasted tab-separated data\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load dataset from pasted data: {e}\")\n",
        "        else:\n",
        "            print(\"No dataset path provided or data pasted. Please provide a dataset.\")\n",
        "            return\n",
        "\n",
        "        # Perform a check for NaNs or any other required processing here\n",
        "        check_for_nans(dataset_df, \"your dataset\")\n",
        "\n",
        "        display(dataset_df.head())\n",
        "\n",
        "# Set the button click event\n",
        "load_button.on_click(load_data)\n",
        "\n",
        "# Display the widgets\n",
        "display(widgets.VBox([dataset_path_input, results_folder_input, data_textarea, load_button, output]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9Wm3BIHnuI"
      },
      "source": [
        "## **1.4. Map your data**\n",
        "---\n",
        "\n",
        "## Required Columns\n",
        "\n",
        "<font size = 4>To plot your data, we need to ensure the presence of specific columns in the dataset. Here's a breakdown of the required columns:\n",
        "\n",
        "- **`Condition`**: Identifies the biological condition.\n",
        "\n",
        "- **`Repeat`**: Represents the biological repeat.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VE9oqIEHrG-"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Map your dataset:\n",
        "\n",
        "\n",
        "import ipywidgets as widgets  # Ensure we have the required widgets module imported\n",
        "import pandas as pd\n",
        "\n",
        "save_path = os.path.join(Results_Folder, 'dataset_df.csv')\n",
        "\n",
        "\n",
        "def single_stage_column_mapping(df):\n",
        "    # Define the columns we need to map: Condition, Repeat\n",
        "    mappings = {\n",
        "        'Condition': 'Identifies the biological conditions.',\n",
        "        'Repeat': 'Represents the biological repeats.'\n",
        "    }\n",
        "\n",
        "    dropdowns = {}\n",
        "    for key, description in mappings.items():\n",
        "        description_label = widgets.Label(f\"{key} ({description}):\")\n",
        "        dropdowns[key] = widgets.Dropdown(options=df.columns, layout=widgets.Layout(width='250px'))\n",
        "\n",
        "        # Use HBox to display the description label next to the dropdown\n",
        "        hbox = widgets.HBox([description_label, dropdowns[key]])\n",
        "        display(hbox)\n",
        "\n",
        "    confirm_button = widgets.Button(description=\"Confirm Mappings\")\n",
        "\n",
        "    def confirm_mappings(button):\n",
        "        # Perform the mapping based on the user selection\n",
        "        column_mapping = {dropdown.value: key for key, dropdown in dropdowns.items()}\n",
        "        new_df = df.rename(columns=column_mapping)\n",
        "\n",
        "        print(\"Columns Mapped Successfully!\")\n",
        "\n",
        "        # Count and print unique conditions\n",
        "        unique_conditions = new_df['Condition'].unique()\n",
        "        print(f\"Number of unique conditions: {len(unique_conditions)}\")\n",
        "        print(\"Conditions:\", \", \".join(unique_conditions))\n",
        "\n",
        "        # Count and print biological repeats\n",
        "        unique_repeats = new_df['Repeat'].unique()\n",
        "        print(f\"Number of biological repeats: {len(unique_repeats)}\")\n",
        "        print(\"Repeats:\", \", \".join(map(str, unique_repeats)))\n",
        "\n",
        "\n",
        "        # Check that each biological condition has exactly the same repeat names\n",
        "        condition_repeats = new_df.groupby('Condition')['Repeat'].apply(set)\n",
        "        if len(set(map(frozenset, condition_repeats))) == 1:\n",
        "            print(\"All biological conditions have exactly the same repeat names.\")\n",
        "        else:\n",
        "            print(\"Warning: Not all biological conditions have the same repeat names.\")\n",
        "\n",
        "        # Update the global dataset_df with the new mappings\n",
        "        global dataset_df\n",
        "        dataset_df = new_df\n",
        "        save_dataframe_with_progress(dataset_df, save_path)\n",
        "\n",
        "\n",
        "    confirm_button.on_click(confirm_mappings)\n",
        "    display(confirm_button)\n",
        "\n",
        "single_stage_column_mapping(dataset_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.5. Normalize your data (optional)**\n",
        "\n",
        "This cell below normalizes the numerical columns in your dataset by dividing each value by the average of a selected control condition within each repeat. To use it, select your desired control condition from the dropdown menu. The normalized values will be added to your dataset and saved automatically.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6racjS2a316"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Normalise your dataset:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "save_path = os.path.join(Results_Folder, 'dataset_df.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Create a dropdown widget for selecting the control condition\n",
        "unique_conditions = dataset_df['Condition'].unique()\n",
        "control_dropdown = widgets.Dropdown(\n",
        "    options=unique_conditions,\n",
        "    description='Select Control Condition:',\n",
        "    value=unique_conditions[0],\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Function to normalize data based on selected control condition\n",
        "def normalize_data(control_condition):\n",
        "    global dataset_df  # Declare dataset_df as global to modify it\n",
        "\n",
        "    # Ensure 'Condition' and 'Repeat' are treated as strings\n",
        "    dataset_df['Condition'] = dataset_df['Condition'].astype(str)\n",
        "    dataset_df['Repeat'] = dataset_df['Repeat'].astype(str)\n",
        "\n",
        "    # Step 2: Identify numerical columns excluding 'Condition' and 'Repeat'\n",
        "    numerical_columns = dataset_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    columns_to_normalize = [col for col in numerical_columns if col not in ['Condition', 'Repeat']]\n",
        "\n",
        "    # Remove previous control mean and normalized columns if they exist\n",
        "    control_mean_cols = [col + '_control_mean' for col in columns_to_normalize]\n",
        "    norm_cols = [col + ' norm' for col in columns_to_normalize]\n",
        "    cols_to_drop = [col for col in control_mean_cols + norm_cols if col in dataset_df.columns]\n",
        "    if cols_to_drop:\n",
        "        dataset_df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    # Step 3: Compute control means for each 'Repeat'\n",
        "    control_means = (\n",
        "        dataset_df[dataset_df['Condition'] == control_condition]\n",
        "        .groupby('Repeat')[columns_to_normalize]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Rename columns to indicate they are control means\n",
        "    control_mean_columns = {col: col + '_control_mean' for col in columns_to_normalize}\n",
        "    control_means.rename(columns=control_mean_columns, inplace=True)\n",
        "\n",
        "    # Step 4: Merge control means back to dataset_df\n",
        "    dataset_df = dataset_df.merge(control_means, on='Repeat', how='left')\n",
        "\n",
        "    # Step 5: Normalize and create new columns\n",
        "    for col in columns_to_normalize:\n",
        "        control_mean_col = col + '_control_mean'\n",
        "        norm_col = col + ' norm'\n",
        "        dataset_df[norm_col] = dataset_df[col] / dataset_df[control_mean_col]\n",
        "\n",
        "    # Optionally, drop control mean columns if no longer needed\n",
        "    # dataset_df.drop(columns=control_mean_cols, inplace=True)\n",
        "\n",
        "    # Display the updated DataFrame\n",
        "    clear_output()\n",
        "    display(control_dropdown)\n",
        "    display(dataset_df.head())\n",
        "    save_dataframe_with_progress(dataset_df, save_path)\n",
        "\n",
        "# Link the dropdown widget to the normalization function\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        normalize_data(change['new'])\n",
        "\n",
        "control_dropdown.observe(on_change)\n",
        "\n",
        "# Display the dropdown widget\n",
        "display(control_dropdown)\n",
        "\n",
        "# Initial normalization with default selection\n",
        "normalize_data(control_dropdown.value)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nw-3KC1_TzQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11aD1AmQh7ST"
      },
      "source": [
        "-------------------------------------------\n",
        "\n",
        "# **Part 2. Plot your entire dataset**\n",
        "-------------------------------------------\n",
        "\n",
        "<font size = 4> In this section you can plot your data. Data and graphs are automatically saved in your result folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pjRDlYOgr3r"
      },
      "source": [
        "## **2.1. Plot your entire dataset**\n",
        "--------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIa_MlR2Ktu-"
      },
      "source": [
        "##**Statistical analyses**\n",
        "### Cohen's d (Effect Size):\n",
        "<font size = 4>Cohen's d measures the size of the difference between two groups, normalized by their pooled standard deviation. Values can be interpreted as small (0 to 0.2), medium (0.2 to 0.5), or large (0.5 and above) effects. It helps quantify how significant the observed difference is, beyond just being statistically significant.\n",
        "\n",
        "### Randomization Test:\n",
        "<font size = 4>This non-parametric test evaluates if observed differences between conditions could have arisen by random chance. It shuffles condition labels multiple times, recalculating the Cohen's d each time. The resulting p-value, which indicates the likelihood of observing the actual difference by chance, provides evidence against the null hypothesis: a smaller p-value implies stronger evidence against the null.\n",
        "\n",
        "### Bonferroni Correction:\n",
        "<font size = 4>Given multiple comparisons, the Bonferroni Correction adjusts significance thresholds to mitigate the risk of false positives. By dividing the standard significance level (alpha) by the number of tests, it ensures that only robust findings are considered significant. However, it's worth noting that this method can be conservative, sometimes overlooking genuine effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HTMbI68Heyis"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot (entire dataset)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from matplotlib.ticker import FixedLocator\n",
        "import itertools\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Plots\"  # Change to your actual folder path\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = dataset_df  # Change to your actual dataframe variable\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "    # Select only numerical columns\n",
        "    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def create_condition_selector(df, column_name):\n",
        "    conditions = df[column_name].unique()\n",
        "    condition_selector = SelectMultiple(\n",
        "        options=conditions,\n",
        "        description='Conditions:',\n",
        "        disabled=False,\n",
        "        layout=Layout(width='100%')  # Adjusting the layout width\n",
        "    )\n",
        "    return condition_selector\n",
        "\n",
        "def display_condition_selection(df, column_name):\n",
        "    condition_selector = create_condition_selector(df, column_name)\n",
        "\n",
        "    condition_accordion = Accordion(children=[VBox([condition_selector])])\n",
        "    condition_accordion.set_title(0, 'Select Conditions')\n",
        "    display(condition_accordion)\n",
        "    return condition_selector\n",
        "\n",
        "def format_scientific_for_ticks(x):\n",
        "    \"\"\"Format p-values for ticks: use scientific notation for values below 0.001, otherwise use standard notation.\"\"\"\n",
        "    if x < 0.001:\n",
        "        return f\"{x:.1e}\"\n",
        "    else:\n",
        "        return f\"{x:.4f}\"\n",
        "\n",
        "def format_p_value(x):\n",
        "    \"\"\"Format p-values to four significant digits.\"\"\"\n",
        "    if x < 0.001:\n",
        "        return \"< 0.001\"\n",
        "    else:\n",
        "        return f\"{x:.4g}\"  # .4g ensures four significant digits\n",
        "\n",
        "def safe_log10_p_values(matrix):\n",
        "    \"\"\"Apply a safe logarithmic transformation to p-values, handling p=1 specifically.\"\"\"\n",
        "    # Replace non-positive values with a very small number just greater than 0\n",
        "    small_value = np.nextafter(0, 1)\n",
        "    adjusted_matrix = np.where(matrix > 0, matrix, small_value)\n",
        "\n",
        "    logged_matrix = -np.log10(adjusted_matrix)\n",
        "    logged_matrix[matrix == 1] = -np.log10(0.999)\n",
        "    return logged_matrix\n",
        "\n",
        "def plot_heatmap(ax, matrix, title, cmap='viridis'):\n",
        "    \"\"\"Plot a heatmap with logarithmic scaling of p-values and real p-values as annotations.\n",
        "    Skip annotations if there are more than 7 conditions.\"\"\"\n",
        "    log_matrix = safe_log10_p_values(matrix.fillna(1))\n",
        "\n",
        "    # Define the normalization range\n",
        "    vmin = -np.log10(0.1)  # Set vmin to the log-transformed value of 0.1\n",
        "    vmax = np.max(log_matrix[np.isfinite(log_matrix)])\n",
        "\n",
        "    if vmin > vmax:\n",
        "        vmin = vmax\n",
        "\n",
        "    # Format annotations if conditions are 6 or fewer\n",
        "    num_conditions = len(matrix.columns)\n",
        "    if num_conditions <= 7:\n",
        "        formatted_annotations = matrix.applymap(lambda x: format_p_value(x) if pd.notna(x) else \"NaN\")\n",
        "    else:\n",
        "        formatted_annotations = False  # No annotations\n",
        "\n",
        "    # Plot the heatmap without the color bar\n",
        "    heatmap = sns.heatmap(log_matrix, ax=ax, cmap=cmap, annot=formatted_annotations,\n",
        "                          fmt=\"\", xticklabels=matrix.columns, yticklabels=matrix.index, cbar=False, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "    # Create a color bar with conditional formatting for ticks\n",
        "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = ax.figure.colorbar(sm, ax=ax)\n",
        "\n",
        "    # Set custom ticks and labels for the color bar\n",
        "    num_ticks = 5\n",
        "    tick_locs = np.linspace(vmin, vmax, num_ticks)\n",
        "    tick_labels = [format_scientific_for_ticks(10**-tick) for tick in tick_locs]\n",
        "    cbar.set_ticks(tick_locs)\n",
        "    cbar.set_ticklabels(tick_labels)\n",
        "\n",
        "\n",
        "def cohen_d(group1, group2):\n",
        "    \"\"\"Calculate Cohen's d for measuring effect size between two groups.\"\"\"\n",
        "    diff = group1.mean() - group2.mean()\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1 = group1.var(ddof=1)  # ddof=1 for sample variance\n",
        "    var2 = group2.var(ddof=1)\n",
        "    pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    d = diff / np.sqrt(pooled_var)\n",
        "    return d\n",
        "\n",
        "def perform_randomization_test(df, cond1, cond2, var, n_iterations=1000):\n",
        "    \"\"\"Perform a randomization test using Cohen's d as the effect size metric.\"\"\"\n",
        "    group1 = df[df['Condition'] == cond1][var]\n",
        "    group2 = df[df['Condition'] == cond2][var]\n",
        "    observed_effect_size = cohen_d(group1, group2)\n",
        "    combined = np.concatenate([group1, group2])\n",
        "    count_extreme = 0\n",
        "    # Perform the randomization test\n",
        "    for i in range(n_iterations):\n",
        "      if i % 100 == 0:\n",
        "        np.random.shuffle(combined)\n",
        "        new_group1 = combined[:len(group1)]\n",
        "        new_group2 = combined[len(group1):]\n",
        "        new_effect_size = cohen_d(new_group1, new_group2)\n",
        "      if abs(new_effect_size) >= abs(observed_effect_size):\n",
        "          count_extreme += 1\n",
        "\n",
        "    p_value = (count_extreme + 1) / (n_iterations + 1)\n",
        "    return p_value\n",
        "\n",
        "def perform_t_test(df, cond1, cond2, var):\n",
        "    \"\"\"Perform a t-test using the average of each repeat for the given conditions and calculate Cohen's d.\"\"\"\n",
        "    group1 = df[df['Condition'] == cond1].groupby('Repeat')[var].mean()\n",
        "    group2 = df[df['Condition'] == cond2].groupby('Repeat')[var].mean()\n",
        "\n",
        "    # Perform the t-test on these group means\n",
        "    t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)  # Use Welch's t-test for unequal variances\n",
        "\n",
        "    return p_value\n",
        "\n",
        "def plot_selected_vars(button, df, Conditions, Results_Folder, condition_selector, stat_method_selector):\n",
        "    plt.clf()  # Clear the current figure before creating a new plot\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "    # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    print(f\"Variables to plot: {variables_to_plot}\")\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "    # Get selected conditions\n",
        "    selected_conditions = condition_selector.value\n",
        "    print(f\"Selected conditions: {selected_conditions}\")\n",
        "    selected_conditions = condition_selector.value\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "    if n_selected_conditions == 0:\n",
        "        print(\"No conditions selected for plotting, therefore all available conditions are selected by default\")\n",
        "        selected_conditions = df[Conditions].unique().tolist()\n",
        "\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "\n",
        "    # Use only selected and ordered conditions\n",
        "    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n",
        "    print(f\"Filtered dataframe shape: {filtered_df.shape}\")\n",
        "    unique_conditions = filtered_df[Conditions].unique().tolist()\n",
        "    print(f\"Unique conditions: {unique_conditions}\")\n",
        "    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n",
        "    n_iterations = 1000\n",
        "    method = stat_method_selector.value\n",
        "    print(f\"Selected method: {method}\")\n",
        "\n",
        "    effect_size_matrices = {}\n",
        "    p_value_matrices = {}\n",
        "    bonferroni_matrices = {}\n",
        "\n",
        "    for var in variables_to_plot:\n",
        "        print(f\"Processing variable: {var}\")\n",
        "        effect_size_matrices[var] = pd.DataFrame(0, index=unique_conditions, columns=unique_conditions)\n",
        "        p_value_matrices[var] = pd.DataFrame(1, index=unique_conditions, columns=unique_conditions)\n",
        "        bonferroni_matrices[var] = pd.DataFrame(1, index=unique_conditions, columns=unique_conditions)\n",
        "\n",
        "        for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n",
        "            group1 = filtered_df[filtered_df[Conditions] == cond1][var]\n",
        "            group2 = filtered_df[filtered_df[Conditions] == cond2][var]\n",
        "\n",
        "            effect_size = abs(cohen_d(group1, group2))\n",
        "            print(f\"Effect size (Cohen's d): {effect_size}\")\n",
        "\n",
        "            if method == 't-test':\n",
        "                p_value = perform_t_test(filtered_df, cond1, cond2, var)\n",
        "            elif method == 'randomization test':\n",
        "                p_value = perform_randomization_test(filtered_df, cond1, cond2, var, n_iterations=n_iterations)\n",
        "\n",
        "            # Set and mirror effect sizes and p-values\n",
        "            effect_size_matrices[var].loc[cond1, cond2] = effect_size_matrices[var].loc[cond2, cond1] = effect_size\n",
        "            p_value_matrices[var].loc[cond1, cond2] = p_value_matrices[var].loc[cond2, cond1] = p_value\n",
        "            bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n",
        "            bonferroni_matrices[var].loc[cond1, cond2] = bonferroni_matrices[var].loc[cond2, cond1] = bonferroni_corrected_p_value\n",
        "\n",
        "        # Save to CSV\n",
        "        combined_df = pd.concat([\n",
        "            effect_size_matrices[var].rename(columns=lambda x: f\"{x} (Effect Size)\"),\n",
        "            p_value_matrices[var].rename(columns=lambda x: f\"{x} ({method} P-Value)\"),\n",
        "            bonferroni_matrices[var].rename(columns=lambda x: f\"{x} ({method} Bonferroni-corrected P-Value)\")\n",
        "        ], axis=1)\n",
        "\n",
        "        combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n",
        "        print(f\"Saved statistics to CSV for variable: {var}\")\n",
        "\n",
        "        # Create a new figure\n",
        "        fig = plt.figure(figsize=(16, 10))\n",
        "        gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n",
        "        ax_box = fig.add_subplot(gs[0, :])\n",
        "\n",
        "        # Extract the data for this variable\n",
        "        data_for_var = df[[Conditions, var, 'Repeat']]\n",
        "        # Save the data_for_var to a CSV for replotting\n",
        "        data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n",
        "\n",
        "        # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n",
        "        Q1 = df[var].quantile(0.2)\n",
        "        Q3 = df[var].quantile(0.8)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define bounds for the outliers\n",
        "        multiplier = 10\n",
        "        lower_bound = Q1 - multiplier * IQR\n",
        "        upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "        # Plotting\n",
        "        sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n",
        "        sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "        ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n",
        "        ax_box.set_title(f\"{var}\")\n",
        "        ax_box.set_xlabel('Condition')\n",
        "        ax_box.set_ylabel(var)\n",
        "        tick_labels = ax_box.get_xticklabels()\n",
        "        tick_locations = ax_box.get_xticks()\n",
        "        ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n",
        "        ax_box.set_xticklabels(tick_labels, rotation=90)\n",
        "        ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "        # Statistical Analyses and Heatmaps\n",
        "\n",
        "        # Effect Size heatmap\n",
        "        ax_d = fig.add_subplot(gs[1, 0])\n",
        "        ax_d.set_xticklabels(ax_d.get_xticklabels(), rotation=90)\n",
        "        sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n",
        "        ax_d.set_title(f\"Effect Size (Cohen's d)\")\n",
        "\n",
        "        # p-value heatmap using the new function\n",
        "        ax_p = fig.add_subplot(gs[1, 1])\n",
        "        plot_heatmap(ax_p, p_value_matrices[var], f\"{method} p-value\")\n",
        "\n",
        "        # Bonferroni corrected p-value heatmap using the new function\n",
        "        ax_bonf = fig.add_subplot(gs[1, 2])\n",
        "        plot_heatmap(ax_bonf, bonferroni_matrices[var], \"Bonferroni-corrected p-value\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n",
        "        pdf_pages.savefig(fig)\n",
        "        pdf_pages.close()\n",
        "        print(f\"Saved PDF for variable: {var}\")\n",
        "        plt.show()\n",
        "\n",
        "# Initialize UI elements\n",
        "selectable_columns = get_selectable_columns(df_to_plot)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "condition_selector = display_condition_selection(df_to_plot, Conditions)\n",
        "stat_method_selector = widgets.Dropdown(\n",
        "    options=['randomization test', 't-test'],\n",
        "    value='randomization test',\n",
        "    description='Stat Method:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars(b, df_to_plot, Conditions, base_folder, condition_selector, stat_method_selector))\n",
        "\n",
        "display(VBox([stat_method_selector, button]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLn_OF4689rS"
      },
      "source": [
        "## **2.2. Export data summaries**\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7ycQAd7HKP3l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "import os\n",
        "\n",
        "# @title ##Export the data summaries\n",
        "\n",
        "\n",
        "# Assuming Results_Folder, dataset_df, and Conditions are defined\n",
        "save_path = f\"{Results_Folder}/variables_summary\"\n",
        "df_to_plot = dataset_df  # Assuming dataset_df is the DataFrame to work with\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "\n",
        "def generate_display_and_save_statistics(df, columns, Conditions, save_path):\n",
        "    \"\"\"\n",
        "    Generates, displays using prettytable, and saves as CSV the statistical summaries\n",
        "    for selected columns of the DataFrame, grouped by the specified condition column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to analyze.\n",
        "    - columns: List of column names to generate statistics for.\n",
        "    - Conditions: Column name to group by.\n",
        "    - save_path: Directory path where CSV files will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the save directory exists\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for var in columns:\n",
        "        if var in df.columns:\n",
        "            # Compute descriptive statistics and additional metrics\n",
        "            grouped_stats = df.groupby(Conditions)[var].describe()\n",
        "            variance = df.groupby(Conditions)[var].var().rename('variance')\n",
        "            skewness = df.groupby(Conditions)[var].skew().rename('skewness')\n",
        "            kurtosis = df.groupby(Conditions)[var].apply(pd.DataFrame.kurt).rename('kurtosis')\n",
        "\n",
        "            # Concatenate all statistics into a single DataFrame\n",
        "            all_stats = pd.concat([grouped_stats, variance, skewness, kurtosis], axis=1)\n",
        "\n",
        "            # Save the summary to a CSV file\n",
        "            csv_filename = f\"{var}_summary.csv\"\n",
        "            all_stats.to_csv(os.path.join(save_path, csv_filename))\n",
        "            print(f\"Saved statistical summary for {var} to {csv_filename}\")\n",
        "\n",
        "            # Initialize PrettyTable, using the DataFrame's columns as table fields\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"Condition\"] + list(all_stats.columns)\n",
        "\n",
        "            # Populate the table with data\n",
        "            for condition, row in all_stats.iterrows():\n",
        "                table.add_row([condition] + row.tolist())\n",
        "\n",
        "            # Set table alignment, style, etc.\n",
        "            table.align = 'r'\n",
        "            print(f\"Statistical Summary for {var}:\\n{table}\")\n",
        "\n",
        "generate_display_and_save_statistics(df_to_plot, selectable_columns, Conditions, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHi3-zp3JTd"
      },
      "source": [
        "--------\n",
        "# **Part 3. Plot a balanced dataset (batch correction)**\n",
        "--------\n",
        "\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqu5RWoh8TNc"
      },
      "source": [
        "## **3.1. Assess if your dataset is balanced**\n",
        "---\n",
        "\n",
        "In data analyses, the balance of the dataset is important, particularly in ensuring that each biological repeat carries equal weight. Here's why this balance is essential:\n",
        "\n",
        "### Accurate Representation of Biological Variability\n",
        "\n",
        "- **Capturing True Biological Variation**: Biological repeats are crucial for capturing the natural variability inherent in biological systems. Equal weighting ensures that this variability is accurately represented.\n",
        "- **Reducing Sampling Bias**: By balancing the dataset, we avoid overemphasizing the characteristics of any single repeat, which might not be representative of the broader biological context.\n",
        "\n",
        "If your data is too imbalanced, it may be useful to ensure that this does not shift your results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dlqqoWZlzoeL"
      },
      "outputs": [],
      "source": [
        "# @title ##Check the number of values per condition per repeats\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "\n",
        "def count_values_by_condition_and_repeat(df, Results_Folder, condition_col='Condition', repeat_col='Repeat'):\n",
        "    \"\"\"\n",
        "    Counts the number of rows for each combination of condition and repeat, plots a histogram,\n",
        "    and adds the counts on top of each bar in the graph.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the results folder exists\n",
        "    Balanced_dataset_folder = os.path.join(Results_Folder, \"Balanced_dataset\")\n",
        "    if not os.path.exists(Balanced_dataset_folder):\n",
        "        os.makedirs(Balanced_dataset_folder)\n",
        "\n",
        "    # Counting rows per condition and repeat\n",
        "    value_counts = df.groupby([condition_col, repeat_col]).size().reset_index(name='Number_of_Values')\n",
        "\n",
        "    # Preparing data for plotting\n",
        "    pivot_df = value_counts.pivot(index=condition_col, columns=repeat_col, values='Number_of_Values').fillna(0)\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax)\n",
        "    ax.set_xlabel('Condition')\n",
        "    ax.set_ylabel('Number of Values')\n",
        "    ax.set_title('Stacked Histogram of Value Counts per Condition and Repeat')\n",
        "    ax.legend(title=repeat_col)\n",
        "\n",
        "    # Adding counts on top of each bar\n",
        "    for bar in bars.patches:\n",
        "        height = bar.get_height()\n",
        "        if height > 0:  # Only annotate non-zero heights to avoid clutter\n",
        "            ax.annotate(f'{int(height)}',\n",
        "                        (bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n",
        "                        ha='center', va='center', color='white', size=9)\n",
        "\n",
        "    # Save the plot as a PDF in the QC subfolder\n",
        "    pdf_file = os.path.join(Balanced_dataset_folder, 'Value_Counts_Histogram.pdf')\n",
        "    plt.savefig(pdf_file, bbox_inches='tight')\n",
        "    print(f\"Saved histogram to {pdf_file}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming dataset_df is your DataFrame and Results_Folder is defined\n",
        "# result_df = count_values_by_condition_and_repeat(dataset_df, Results_Folder)\n",
        "\n",
        "result_df = count_values_by_condition_and_repeat(dataset_df, Results_Folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FJT4P_Xgr35"
      },
      "source": [
        "## **3.2. Downsample your dataset to ensure that it is balanced**\n",
        "--------\n",
        "\n",
        "### Downsampling and Balancing Dataset\n",
        "\n",
        "This section of the notebook is dedicated to addressing imbalances in the dataset, which is crucial for ensuring the accuracy and reliability of the analysis. The cell bellow will downsample the dataset to balance the number of tracks across different conditions and repeats. It allows for reproducibility by including a `random_seed` parameter, which is set to 42 by default but can be adjusted as needed.\n",
        "\n",
        "All results from this section will be saved in the Balanced Dataset Directory created in your `Results_Folder`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qnJ7W-mCAih_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# @title ##Run this cell to downsample and balance your dataset\n",
        "\n",
        "random_seed = 42  # @param {type: \"number\"}\n",
        "\n",
        "if not os.path.exists(f\"{Results_Folder}/Balanced_dataset\"):\n",
        "    os.makedirs(f\"{Results_Folder}/Balanced_dataset\")\n",
        "\n",
        "def balance_dataset(df, condition_col='Condition', repeat_col='Repeat', random_seed=None):\n",
        "    \"\"\"\n",
        "    Balances the dataset by downsampling rows for each condition and repeat combination.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The DataFrame containing the data.\n",
        "    condition_col (str): The name of the column representing the condition.\n",
        "    repeat_col (str): The name of the column representing the repeat.\n",
        "    random_seed (int, optional): The seed for the random number generator. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A new DataFrame with balanced row counts.\n",
        "    \"\"\"\n",
        "    # Group by condition and repeat, and find the minimum row count for any group\n",
        "    min_row_count = df.groupby([condition_col, repeat_col]).size().min()\n",
        "\n",
        "    # Function to sample min_row_count rows from each group\n",
        "    def sample_rows(group):\n",
        "        return group.sample(n=min_row_count, random_state=random_seed)\n",
        "\n",
        "    # Apply sampling to each group and concatenate the results\n",
        "    balanced_df = df.groupby([condition_col, repeat_col], group_keys=False).apply(sample_rows)\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# Apply the function to your dataset\n",
        "balanced_dataset_df = balance_dataset(dataset_df, random_seed=random_seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phuPCeabgr35"
      },
      "source": [
        "## **3.3. Check if the downsampling has affected data distribution**\n",
        "--------\n",
        "\n",
        "This section of the notebook generates a heatmap visualizing the Kolmogorov-Smirnov (KS) p-values for each numerical column in the dataset, comparing the distributions before and after downsampling. This heatmap serves as a tool for assessing the impact of downsampling on data quality, guiding decisions on whether the downsampled dataset is suitable for further analysis.\n",
        "\n",
        "#### Purpose of the Heatmap\n",
        "- **KS Test:** The KS test is used to determine if two samples are drawn from the same distribution. In this context, it compares the distribution of each numerical column in the original dataset (`merged_tracks_df`) with its counterpart in the downsampled dataset (`balanced_merged_tracks_df`).\n",
        "- **P-Value Interpretation:** The p-value indicates the probability that the two samples come from the same distribution. A higher p-value suggests a greater likelihood that the distributions are similar.\n",
        "\n",
        "#### Interpreting the Heatmap\n",
        "- **Color Coding:** The heatmap uses a color gradient (from viridis) to represent the range of p-values. Darker colors indicate higher p-values.\n",
        "- **P-Value Thresholds:**\n",
        "  - **High P-Values (Lighter Areas):** Indicate that the downsampling process likely did not significantly alter the distribution of that numerical column for the specific condition-repeat group.\n",
        "  - **Low P-Values (Darker Areas):** Suggest that the downsampling process may have affected the distribution significantly.\n",
        "- **Varying P-Values:** Variations in color across different columns and rows help identify which specific numerical columns and condition-repeat groups are most affected by the downsampling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FLVzJH3ogr36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# @title ##Check if your downsampling has affected your data distribution\n",
        "\n",
        "def calculate_ks_p_value(df1, df2, column):\n",
        "    \"\"\"\n",
        "    Calculate the KS p-value for a given column between two dataframes.\n",
        "\n",
        "    Parameters:\n",
        "    df1 (pandas.DataFrame): Original DataFrame.\n",
        "    df2 (pandas.DataFrame): DataFrame after downsampling.\n",
        "    column (str): Column name to compare.\n",
        "\n",
        "    Returns:\n",
        "    float: KS p-value.\n",
        "    \"\"\"\n",
        "    return ks_2samp(df1[column].dropna(), df2[column].dropna())[1]\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_columns = dataset_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Initialize a DataFrame to store KS p-values\n",
        "ks_p_values = pd.DataFrame(columns=numerical_columns)\n",
        "\n",
        "# Iterate over each group and numerical column\n",
        "for group, group_df in dataset_df.groupby(['Condition', 'Repeat']):\n",
        "    group_p_values = []\n",
        "    balanced_group_df = balanced_dataset_df[(balanced_dataset_df['Condition'] == group[0]) & (balanced_dataset_df['Repeat'] == group[1])]\n",
        "    for column in numerical_columns:\n",
        "        p_value = calculate_ks_p_value(group_df, balanced_group_df, column)\n",
        "        group_p_values.append(p_value)\n",
        "    ks_p_values.loc[f'Condition: {group[0]}, Repeat: {group[1]}'] = group_p_values\n",
        "\n",
        "# Maximum number of columns per heatmap\n",
        "max_columns_per_heatmap = 20\n",
        "\n",
        "# Total number of columns\n",
        "total_columns = len(ks_p_values.columns)\n",
        "\n",
        "# Calculate the number of heatmaps needed\n",
        "num_heatmaps = -(-total_columns // max_columns_per_heatmap)  # Ceiling division\n",
        "\n",
        "# File path for the PDF\n",
        "pdf_filepath = Results_Folder+'/Balanced_dataset/p-Value Heatmap.pdf'\n",
        "\n",
        "# Create a PDF file\n",
        "with PdfPages(pdf_filepath) as pdf:\n",
        "    # Loop through each subset of columns and create a heatmap\n",
        "    for i in range(num_heatmaps):\n",
        "        start_col = i * max_columns_per_heatmap\n",
        "        end_col = min(start_col + max_columns_per_heatmap, total_columns)\n",
        "\n",
        "        # Subset of columns for this heatmap\n",
        "        subset_columns = ks_p_values.columns[start_col:end_col]\n",
        "\n",
        "        # Create the heatmap for the subset of columns\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(ks_p_values[subset_columns], cmap='viridis', vmax=0.5, vmin=0)\n",
        "        plt.title(f'Kolmogorov-Smirnov P-Value Heatmap (Columns {start_col+1} to {end_col})')\n",
        "        plt.xlabel('Numerical Columns')\n",
        "        plt.ylabel('Condition-Repeat Groups')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the current figure to the PDF\n",
        "        pdf.savefig()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "print(f\"Saved all heatmaps to {pdf_filepath}\")\n",
        "\n",
        "# Save the p-values to a CSV file\n",
        "ks_p_values.to_csv(Results_Folder + '/Balanced_dataset/ks_p_values.csv')\n",
        "print(\"Saved KS p-values to ks_p_values.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2eUxF87gr36"
      },
      "source": [
        "## **3.4. Plot your balanced dataset**\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ssuVkgP4a3Hf"
      },
      "outputs": [],
      "source": [
        "# @title ##Plot track parameters (balanced dataset)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, VBox, Button, Accordion, SelectMultiple, IntText\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib.ticker import FixedLocator\n",
        "\n",
        "\n",
        "# Parameters to adapt in function of the notebook section\n",
        "base_folder = f\"{Results_Folder}/Balanced_dataset/track_parameters_plots\"\n",
        "Conditions = 'Condition'\n",
        "df_to_plot = balanced_dataset_df\n",
        "\n",
        "# Check and create necessary directories\n",
        "folders = [\"pdf\", \"csv\"]\n",
        "for folder in folders:\n",
        "    dir_path = os.path.join(base_folder, folder)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_selectable_columns(df):\n",
        "    # Exclude certain columns from being plotted\n",
        "    exclude_cols = ['Condition', 'experiment_nb', 'File_name', 'Repeat', 'Unique_ID', 'LABEL', 'TRACK_INDEX', 'TRACK_ID', 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION', 'Exemplar','TRACK_STOP', 'TRACK_START', 'Cluster_UMAP', 'Cluster_tsne']\n",
        "    # Select only numerical columns\n",
        "    return [col for col in df.columns if (df[col].dtype.kind in 'biufc') and (col not in exclude_cols)]\n",
        "\n",
        "\n",
        "def display_variable_checkboxes(selectable_columns):\n",
        "    # Create checkboxes for selectable columns\n",
        "    variable_checkboxes = [widgets.Checkbox(value=False, description=col) for col in selectable_columns]\n",
        "\n",
        "    # Display checkboxes in the notebook\n",
        "    display(widgets.VBox([\n",
        "        widgets.Label('Variables to Plot:'),\n",
        "        widgets.GridBox(variable_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(%d, 300px)\" % 3)),\n",
        "    ]))\n",
        "    return variable_checkboxes\n",
        "\n",
        "def create_condition_selector(df, column_name):\n",
        "    conditions = df[column_name].unique()\n",
        "    condition_selector = SelectMultiple(\n",
        "        options=conditions,\n",
        "        description='Conditions:',\n",
        "        disabled=False,\n",
        "        layout=Layout(width='100%')  # Adjusting the layout width\n",
        "    )\n",
        "    return condition_selector\n",
        "\n",
        "def display_condition_selection(df, column_name):\n",
        "    condition_selector = create_condition_selector(df, column_name)\n",
        "\n",
        "    condition_accordion = Accordion(children=[VBox([condition_selector])])\n",
        "    condition_accordion.set_title(0, 'Select Conditions')\n",
        "    display(condition_accordion)\n",
        "    return condition_selector\n",
        "\n",
        "\n",
        "def plot_selected_vars(button, variable_checkboxes, df, Conditions, Results_Folder, condition_selector):\n",
        "\n",
        "    plt.clf()  # Clear the current figure before creating a new plot\n",
        "    print(\"Plotting in progress...\")\n",
        "\n",
        "  # Get selected variables\n",
        "    variables_to_plot = [box.description for box in variable_checkboxes if box.value]\n",
        "    n_plots = len(variables_to_plot)\n",
        "\n",
        "    if n_plots == 0:\n",
        "        print(\"No variables selected for plotting\")\n",
        "        return\n",
        "\n",
        "  # Get selected conditions\n",
        "    selected_conditions = condition_selector.value\n",
        "    n_selected_conditions = len(selected_conditions)\n",
        "\n",
        "    if n_selected_conditions == 0:\n",
        "        print(\"No conditions selected for plotting\")\n",
        "        return\n",
        "\n",
        "# Use only selected and ordered conditions\n",
        "    filtered_df = df[df[Conditions].isin(selected_conditions)].copy()\n",
        "\n",
        "# Initialize matrices to store effect sizes and p-values for each variable\n",
        "    effect_size_matrices = {}\n",
        "    p_value_matrices = {}\n",
        "    bonferroni_matrices = {}\n",
        "\n",
        "    unique_conditions = filtered_df[Conditions].unique().tolist()\n",
        "    num_comparisons = len(unique_conditions) * (len(unique_conditions) - 1) // 2\n",
        "    alpha = 0.05\n",
        "    corrected_alpha = alpha / num_comparisons\n",
        "    n_iterations = 1000\n",
        "\n",
        "# Loop through each variable to plot\n",
        "    for var in variables_to_plot:\n",
        "\n",
        "      pdf_pages = PdfPages(f\"{Results_Folder}/pdf/{var}_Boxplots_and_Statistics.pdf\")\n",
        "      effect_size_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      p_value_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "      bonferroni_matrix = pd.DataFrame(index=unique_conditions, columns=unique_conditions)\n",
        "\n",
        "      for cond1, cond2 in itertools.combinations(unique_conditions, 2):\n",
        "        group1 = df[df[Conditions] == cond1][var]\n",
        "        group2 = df[df[Conditions] == cond2][var]\n",
        "\n",
        "        original_d = abs(cohen_d(group1, group2))\n",
        "        effect_size_matrix.loc[cond1, cond2] = original_d\n",
        "        effect_size_matrix.loc[cond2, cond1] = original_d  # Mirroring\n",
        "\n",
        "        count_extreme = 0\n",
        "        for i in range(n_iterations):\n",
        "            combined = pd.concat([group1, group2])\n",
        "            shuffled = combined.sample(frac=1, replace=False).reset_index(drop=True)\n",
        "            new_group1 = shuffled[:len(group1)]\n",
        "            new_group2 = shuffled[len(group1):]\n",
        "\n",
        "            new_d = abs(cohen_d(new_group1, new_group2))\n",
        "            if np.abs(new_d) >= np.abs(original_d):\n",
        "                count_extreme += 1\n",
        "\n",
        "        p_value = count_extreme / n_iterations\n",
        "        p_value_matrix.loc[cond1, cond2] = p_value\n",
        "        p_value_matrix.loc[cond2, cond1] = p_value  # Mirroring\n",
        "\n",
        "        # Apply Bonferroni correction\n",
        "        bonferroni_corrected_p_value = min(p_value * num_comparisons, 1.0)\n",
        "        bonferroni_matrix.loc[cond1, cond2] = bonferroni_corrected_p_value\n",
        "        bonferroni_matrix.loc[cond2, cond1] = bonferroni_corrected_p_value  # Mirroring\n",
        "\n",
        "      effect_size_matrices[var] = effect_size_matrix\n",
        "      p_value_matrices[var] = p_value_matrix\n",
        "      bonferroni_matrices[var] = bonferroni_matrix\n",
        "\n",
        "    # Concatenate the three matrices side-by-side\n",
        "      combined_df = pd.concat(\n",
        "        [\n",
        "            effect_size_matrices[var].rename(columns={col: f\"{col} (Effect Size)\" for col in effect_size_matrices[var].columns}),\n",
        "            p_value_matrices[var].rename(columns={col: f\"{col} (P-Value)\" for col in p_value_matrices[var].columns}),\n",
        "            bonferroni_matrices[var].rename(columns={col: f\"{col} (Bonferroni-corrected P-Value)\" for col in bonferroni_matrices[var].columns})\n",
        "        ], axis=1\n",
        "    )\n",
        "\n",
        "    # Save the combined DataFrame to a CSV file\n",
        "      combined_df.to_csv(f\"{Results_Folder}/csv/{var}_statistics_combined.csv\")\n",
        "\n",
        "    # Create a new figure\n",
        "      fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Create a gridspec for 2 rows and 4 columns\n",
        "      gs = GridSpec(2, 3, height_ratios=[1.5, 1])\n",
        "\n",
        "    # Create the ax for boxplot using the gridspec\n",
        "      ax_box = fig.add_subplot(gs[0, :])\n",
        "\n",
        "    # Extract the data for this variable\n",
        "      data_for_var = df[[Conditions, var, 'Repeat' ]]\n",
        "\n",
        "    # Save the data_for_var to a CSV for replotting\n",
        "      data_for_var.to_csv(f\"{Results_Folder}/csv/{var}_boxplot_data.csv\", index=False)\n",
        "\n",
        "    # Calculate the Interquartile Range (IQR) using the 25th and 75th percentiles\n",
        "      Q1 = df[var].quantile(0.25)\n",
        "      Q3 = df[var].quantile(0.75)\n",
        "      IQR = Q3 - Q1\n",
        "\n",
        "    # Define bounds for the outliers\n",
        "      multiplier = 10\n",
        "      lower_bound = Q1 - multiplier * IQR\n",
        "      upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "    # Plotting\n",
        "      sns.boxplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, color='lightgray')  # Boxplot\n",
        "      sns.stripplot(x=Conditions, y=var, data=filtered_df, ax=ax_box, hue='Repeat', dodge=True, jitter=True, alpha=0.2)  # Individual data points\n",
        "      ax_box.set_ylim([max(min(filtered_df[var]), lower_bound), min(max(filtered_df[var]), upper_bound)])\n",
        "      ax_box.set_title(f\"{var}\")\n",
        "      ax_box.set_xlabel('Condition')\n",
        "      ax_box.set_ylabel(var)\n",
        "      tick_labels = ax_box.get_xticklabels()\n",
        "      tick_locations = ax_box.get_xticks()\n",
        "      ax_box.xaxis.set_major_locator(FixedLocator(tick_locations))\n",
        "      ax_box.set_xticklabels(tick_labels, rotation=90)\n",
        "      ax_box.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Repeat')\n",
        "\n",
        "    # Statistical Analyses and Heatmaps\n",
        "\n",
        "    # Effect Size heatmap ax\n",
        "      ax_d = fig.add_subplot(gs[1, 0])\n",
        "      sns.heatmap(effect_size_matrices[var].fillna(0), annot=True, cmap=\"viridis\", cbar=True, square=True, ax=ax_d, vmax=1)\n",
        "      ax_d.set_title(f\"Effect Size (Cohen's d) for {var}\")\n",
        "\n",
        "    # p-value heatmap ax\n",
        "      ax_p = fig.add_subplot(gs[1, 1])\n",
        "      sns.heatmap(p_value_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_p, vmax=0.1)\n",
        "      ax_p.set_title(f\"Randomization Test p-value for {var}\")\n",
        "\n",
        "    # Bonferroni corrected p-value heatmap ax\n",
        "      ax_bonf = fig.add_subplot(gs[1, 2])\n",
        "      sns.heatmap(bonferroni_matrices[var].fillna(1), annot=True, cmap=\"viridis_r\", cbar=True, square=True, ax=ax_bonf, vmax=0.1)\n",
        "      ax_bonf.set_title(f\"Bonferroni-corrected p-value for {var}\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      pdf_pages.savefig(fig)\n",
        "\n",
        "    # Close the PDF\n",
        "      pdf_pages.close()\n",
        "\n",
        "condition_selector = display_condition_selection(df_to_plot, Conditions)\n",
        "selectable_columns = get_selectable_columns(df_to_plot)\n",
        "variable_checkboxes = display_variable_checkboxes(selectable_columns)\n",
        "\n",
        "button = Button(description=\"Plot Selected Variables\", layout=Layout(width='400px'), button_style='info')\n",
        "button.on_click(lambda b: plot_selected_vars(b, variable_checkboxes, df_to_plot, Conditions, base_folder, condition_selector))\n",
        "display(button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KRx9fZGbCeSu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "import os\n",
        "\n",
        "# @title ##Export the data summaries\n",
        "\n",
        "\n",
        "# Assuming Results_Folder, dataset_df, and Conditions are defined\n",
        "save_path = f\"{Results_Folder}/Balanced_dataset/track_parameters_plots\"\n",
        "df_to_plot = balanced_dataset_df  # Assuming dataset_df is the DataFrame to work with\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "\n",
        "def generate_display_and_save_statistics(df, columns, Conditions, save_path):\n",
        "    \"\"\"\n",
        "    Generates, displays using prettytable, and saves as CSV the statistical summaries\n",
        "    for selected columns of the DataFrame, grouped by the specified condition column.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to analyze.\n",
        "    - columns: List of column names to generate statistics for.\n",
        "    - Conditions: Column name to group by.\n",
        "    - save_path: Directory path where CSV files will be saved.\n",
        "    \"\"\"\n",
        "    # Ensure the save directory exists\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    for var in columns:\n",
        "        if var in df.columns:\n",
        "            # Compute descriptive statistics and additional metrics\n",
        "            grouped_stats = df.groupby(Conditions)[var].describe()\n",
        "            variance = df.groupby(Conditions)[var].var().rename('variance')\n",
        "            skewness = df.groupby(Conditions)[var].skew().rename('skewness')\n",
        "            kurtosis = df.groupby(Conditions)[var].apply(pd.DataFrame.kurt).rename('kurtosis')\n",
        "\n",
        "            # Concatenate all statistics into a single DataFrame\n",
        "            all_stats = pd.concat([grouped_stats, variance, skewness, kurtosis], axis=1)\n",
        "\n",
        "            # Save the summary to a CSV file\n",
        "            csv_filename = f\"{var}_summary.csv\"\n",
        "            all_stats.to_csv(os.path.join(save_path, csv_filename))\n",
        "            print(f\"Saved statistical summary for {var} to {csv_filename}\")\n",
        "\n",
        "            # Initialize PrettyTable, using the DataFrame's columns as table fields\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"Condition\"] + list(all_stats.columns)\n",
        "\n",
        "            # Populate the table with data\n",
        "            for condition, row in all_stats.iterrows():\n",
        "                table.add_row([condition] + row.tolist())\n",
        "\n",
        "            # Set table alignment, style, etc.\n",
        "            table.align = 'r'\n",
        "            print(f\"Statistical Summary for {var}:\\n{table}\")\n",
        "\n",
        "generate_display_and_save_statistics(df_to_plot, selectable_columns, Conditions, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmnkq5cG9IDv"
      },
      "source": [
        "# **Part 4. pMoSS (p-value Model using the Sample Size)** (work in progress)\n",
        "--------\n",
        "\n",
        "pMoSS (p-value Model using the Sample Size) is a Python code to model the p-value as an n-dependent function using Monte Carlo cross-validation.\n",
        "\n",
        "This statistical method uses the relationship between the p-value and the sample size to characterize the data of an experiment and decide, robustly, when the null hypothesis can be rejected.\n",
        "\n",
        "The method is presented at [E. Gómez-de-Mariscal, V. Guerrero, A. Sneider, H. Jayatilaka, J. M. Phillip, D. Wirtz and A. Muñoz-Barrutia, \"Use of the p-values as a size-dependent function to address practical differences when analyzing large datasets.\" Scientific Reports, 2021.](https://www.nature.com/articles/s41598-021-00199-5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwzNzfP7QHnK"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/BSEL-UC3M/pMoSS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOVLt-cMQfrr"
      },
      "outputs": [],
      "source": [
        "# Navigate into the repository directory\n",
        "%cd pMoSS\n",
        "\n",
        "# Use find and sed to replace np.str with str in all .py files\n",
        "!find . -type f -name \"*.py\" -exec sed -i 's/np.str/str/g' {} +\n",
        "\n",
        "%cd /content\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/pMoSS')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrCqk8poKnoD"
      },
      "outputs": [],
      "source": [
        "# Load the packages needed to run the scripts in this notebook\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pmoss.analysis import compute_diagnosis\n",
        "from pmoss import create_combination\n",
        "from pmoss.display import scatterplot_decrease_parameters, plot_pcurve_by_measure, composed_plot, table_of_results\n",
        "from pmoss.models.exponential_fit import decission_data_exponential\n",
        "from pmoss.loaders import morphoparam\n",
        "# Avoid warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_TPP7wtKnoE"
      },
      "source": [
        "### Information about the data.\n",
        "Provide path containing the data (csv or excel) and the name of the file.\n",
        "\n",
        "Note: The column identifying the group to which each value belongs to, must have the name \"Condition\" and should be the first column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr-zcMjFMxOT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming dataset_df is already loaded and Results_Folder is defined\n",
        "\n",
        "# Step 1: Reorder Columns - Place 'Condition' column first\n",
        "cols = ['Condition'] + [col for col in dataset_df.columns if col != 'Condition']\n",
        "dataset_df = dataset_df[cols]\n",
        "\n",
        "# Step 2: Remove the 'Repeat' column if it exists\n",
        "if 'Repeat' in dataset_df.columns:\n",
        "    dataset_df = dataset_df.drop(columns=['Repeat'])\n",
        "\n",
        "# Step 3: Remove Non-Numeric Columns except for 'Condition'\n",
        "numeric_cols = dataset_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "dataset_df = dataset_df[['Condition'] + numeric_cols]\n",
        "\n",
        "# Step 4: Save the modified DataFrame as a CSV file\n",
        "file_path = os.path.join(Results_Folder, 'modified_dataset.csv')\n",
        "dataset_df.to_csv(file_path, index=False)\n",
        "\n",
        "file_name = 'modified_dataset.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxZo74jKnoE"
      },
      "source": [
        "### Estimation of the p-value function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNbAN4BPKnoF"
      },
      "source": [
        "Initialization parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHhW58UbKnoF"
      },
      "outputs": [],
      "source": [
        "# number of \"n-values\" to evaluate (size of N-grid)\n",
        "grid_size = 100\n",
        "# minimum \"n-value\" to compute Monte Carlo cross-validation\n",
        "n0 = 2\n",
        "# maximum \"n-value\" to compute Monte Carlo cross-validation\n",
        "Nmax = 1200\n",
        "\n",
        "# This value prevents from having only one iteration for the highest \"n-value\":\n",
        "# final iterations = k*(m/min(m,Nmax)) where m is the size of group with less observations.\n",
        "k = 20\n",
        "\n",
        "# This value prevents from having millions of iterations in n0 (the lowest\"n-value\"):\n",
        "# initial iterations = np.log((m/n0)*initial_portion) where m is the size of group with less observations.\n",
        "initial_portion= 1/3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97lD5IT0KnoF"
      },
      "source": [
        "Parameters for the calculation of the decision index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udIlh7pdKnoF"
      },
      "outputs": [],
      "source": [
        "alpha = 0.05 # alpha for a 100(1-alpha) statistical significance.\n",
        "gamma = 5e-06 # gamma in the paper = gamma*alpha.\n",
        "# Statistitical test to evaluate\n",
        "test = 'MannWhitneyU'\n",
        "# Method to estimate the p-value function\n",
        "method = 'exponential'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BirugxNtKnoF"
      },
      "source": [
        "Estimation of the p-value function and assesment of the decision index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFKUyLNAKnoF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pvalues, param, Theta = compute_diagnosis(file_name, path = Results_Folder, gamma = gamma,\n",
        "                                          alpha = alpha, grid_size = grid_size,\n",
        "                                          n0 = n0, Nmax = Nmax,k = k,\n",
        "                                          initial_portion=initial_portion,\n",
        "                                          method = method, test = test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPhZ2tPwKnoG"
      },
      "source": [
        "Save the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cl6TVeTaKnoG"
      },
      "outputs": [],
      "source": [
        "# Save computed parameters\n",
        "pvalues.to_csv(os.path.join(Results_Folder, \"pMoSS_pvalues.csv\"), index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7xzrLTKnoG"
      },
      "source": [
        "### Plot of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkm5qhHTKnoG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "\n",
        "## Write the path and file_nameif it's different from the previous one or you will compute the analysis from here\n",
        "# path = '../data/morphology/'\n",
        "# file_name = 'Aging morphology data.xlsx'\n",
        "\n",
        "df = pd.read_csv(os.path.join(Results_Folder, \"pMoSS_pvalues.csv\"), sep=',')\n",
        "\n",
        "# Obtain the data, variables and name of the groups for which you would like to get a plot\n",
        "data, variables, group_labels = morphoparam(file_name, path = Results_Folder)\n",
        "\n",
        "# Declare the variables for which you would like to get a plot\n",
        "variables={\n",
        "            '0': 'area (px^2)',\n",
        "            '1': 'short axis length (px)',\n",
        "            '2': 'orientation'\n",
        "            }\n",
        "\n",
        "# You can create all the combinations from a dictionary with the labels of each group, or declare which combinations you want:\n",
        "# 1.- All combinations should be written exactly as in the csv of the p-values.\n",
        "\n",
        "# group_labels = {'0':'A02',\n",
        "#             '1':'A03',\n",
        "#             '2':'A09',\n",
        "#             '3':'A16',\n",
        "#             '4':'A29',\n",
        "#             '5':'A35',\n",
        "#             '6':'A55',\n",
        "#             '7':'A65',\n",
        "#             '8':'A85',\n",
        "#             '9':'A96'\n",
        "#             }\n",
        "#combination = create_combination(group_labels)\n",
        "\n",
        "# 2.- Set the desired combinations\n",
        "combination={\n",
        " '0': 'A02_A03',\n",
        " '1': 'A02_A09',\n",
        " '2': 'A02_A16',\n",
        " '3': 'A02_A29',\n",
        " '4': 'A02_A35',\n",
        " '5': 'A02_A55',\n",
        " '6': 'A02_A65',\n",
        " '7': 'A02_A85',\n",
        " '8': 'A02_A96'\n",
        " }\n",
        "\n",
        "# Load the data related to exponential parameters:\n",
        "\n",
        "# param = pd.read_csv('../data/morphology/aging_morphology_param.csv',sep=',')\n",
        "\n",
        "# or calculate it:\n",
        "param = decission_data_exponential(df, combination, variables, sign_level = 0.05, gamma = 5e-06)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP3Mw8J8KnoG"
      },
      "outputs": [],
      "source": [
        "# print the results:\n",
        "table = table_of_results(param, variables, combination)\n",
        "table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK3RPMKLKnoH"
      },
      "outputs": [],
      "source": [
        "# plot\n",
        "scatterplot_decrease_parameters(df, combination,variables, path = path, fs = 10, width = 5, height = 5,\n",
        "                                plot_type=\"exp-param\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh6227k2KnoH",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "scatterplot_decrease_parameters(df, combination,variables, path = path, fs = 10, width = 5, height = 5,\n",
        "                                plot_type=\"sampled-nalpha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJyAT5ZBKnoH"
      },
      "outputs": [],
      "source": [
        "scatterplot_decrease_parameters(df, combination,variables, path = path, fs = 10, width = 5, height = 5,\n",
        "                                plot_type=\"theory-nalpha\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZv3OKkqKnoH"
      },
      "outputs": [],
      "source": [
        "plot_pcurve_by_measure(df, combination, variables, path = path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc7wq8bDKnoH",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "composed_plot(data, df, group_labels, combination, variables, fs = 23, width = 37, height = 15, bins = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd817UHYhCGD"
      },
      "source": [
        "# **Part 4. Version log**\n",
        "---\n",
        "<font size = 4>While I strive to provide accurate and helpful information, please be aware that:\n",
        "  - This notebook may contain bugs.\n",
        "  - Features are currently limited and will be expanded in future releases.\n",
        "\n",
        "<font size = 4>We encourage users to report any issues or suggestions for improvement.\n",
        "\n",
        "<font size = 4>**Version 0.3**\n",
        "\n",
        "Added the possibility to normalise the dataset\n",
        "\n",
        "\n",
        "<font size = 4>**Version 0.2**\n",
        "\n",
        "Various bug fixes and improvements\n",
        "\n",
        "<font size = 4>**Version 0.1**\n",
        "\n",
        "This is the first release of this notebook.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fmnkq5cG9IDv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}